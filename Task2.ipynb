{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81afeadc",
   "metadata": {},
   "source": [
    "## Task 2 - CLIP Fine-Tuning on the Visual Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eb2e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title GPU / Python / Torch sanity\n",
    "import os, sys, subprocess, json, platform, torch\n",
    "print(\"Python :\", sys.version)\n",
    "print(\"CUDA   :\", torch.version.cuda)\n",
    "print(\"Torch  :\", torch.__version__)\n",
    "print(\"Device :\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n",
    "!nvidia-smi || true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab32194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from transformers import CLIPProcessor, CLIPModel, AutoProcessor, CLIPVisionModel, logging\n",
    "import clip\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from torchinfo import summary\n",
    "from tqdm.autonotebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import json\n",
    "import warnings\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "import io\n",
    "from torchvision.datasets import Flowers102\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0a8323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 降噪：避免 tokenizers 在多工情境下噴警告\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# some settings\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_ID = \"openai/clip-vit-large-patch14\" # pre-trained CLIP model (ViT-L/14)\n",
    "BATCH_SIZE = 64 # adjust based on your GPU memory\n",
    "NUM_WORKERS = 2\n",
    "gradient_accumulation_steps = 1 # adjust based on your GPU memory\n",
    "# For Linear Probe & LoRA\n",
    "NUM_EPOCHS = 1\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "processor = CLIPProcessor.from_pretrained(MODEL_ID)\n",
    "model     = CLIPModel.from_pretrained(MODEL_ID).to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53344f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Flowers102 (torchvision) ====\n",
    "\n",
    "\n",
    "# 把 PIL 影像轉成 (3,224,224) 的 CLIP 規格 tensor\n",
    "def clip_image_transform(image):\n",
    "    if not isinstance(image, Image.Image):\n",
    "        image = Image.fromarray(image)\n",
    "    image = image.convert(\"RGB\")\n",
    "    px = processor(images=image, return_tensors=\"pt\")[\"pixel_values\"][0]  # (3,224,224)\n",
    "    return px\n",
    "\n",
    "flowers102_test_dts = Flowers102(\n",
    "    root=\"./data\",\n",
    "    split=\"test\",\n",
    "    transform=clip_image_transform,\n",
    "    download=True\n",
    ")\n",
    "print(f\"Total test samples (Flowers102): {len(flowers102_test_dts)}\")  # 6149\n",
    "\n",
    "flowers102_test_loader = DataLoader(\n",
    "    flowers102_test_dts,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# 類別名稱（使用你上傳的 cat_to_name.json）\n",
    "with open(\"./data/cat_to_name.json\", \"r\") as f:\n",
    "    cat_to_name = json.load(f)\n",
    "flowers102_class_names = [cat_to_name[str(i)] for i in range(1, 103)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6365bb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== CUB-200-2011 (HF datasets) ====\n",
    "\n",
    "birds_200 = load_dataset(\"bentrevett/caltech-ucsd-birds-200-2011\", cache_dir=\"./data\")\n",
    "cub_bird_test_raw = birds_200[\"test\"]\n",
    "print(f\"Total test samples (CUB): {len(cub_bird_test_raw)}\")  # 5794\n",
    "\n",
    "def _to_pil_image_safe(img):\n",
    "    \"\"\"Robust convert HF dataset image (PIL/ndarray/list/bytes) -> PIL.Image.\"\"\"\n",
    "    if isinstance(img, Image.Image):\n",
    "        return img\n",
    "    if isinstance(img, np.ndarray):\n",
    "        return Image.fromarray(img)\n",
    "    # list may be list-of-ints (bytes) or nested lists (HWC)\n",
    "    if isinstance(img, (list, tuple)):\n",
    "        # list of ints -> bytes\n",
    "        if all(isinstance(x, (int, np.integer)) for x in img):\n",
    "            return Image.open(io.BytesIO(bytes(img)))\n",
    "        # else try to convert to array (may raise ValueError for ragged)\n",
    "        try:\n",
    "            arr = np.asarray(img, dtype=np.uint8)\n",
    "            return Image.fromarray(arr)\n",
    "        except Exception as ex:\n",
    "            raise TypeError(f\"Cannot convert list image to PIL (ragged?): {ex}\") from ex\n",
    "    if isinstance(img, (bytes, bytearray)):\n",
    "        return Image.open(io.BytesIO(img))\n",
    "    raise TypeError(f\"Unsupported image type for conversion to PIL: {type(img)}\")\n",
    "\n",
    "def cub_transform(example):\n",
    "    \"\"\"\n",
    "    Accept both single example and batch-dict (where example['image'] is a list).\n",
    "    Return pixel_values as numpy arrays (C,H,W) to keep HF formatting/collate stable.\n",
    "    \"\"\"\n",
    "    def proc_one(img):\n",
    "        pil = _to_pil_image_safe(img).convert(\"RGB\")\n",
    "        px = processor(images=pil, return_tensors=\"pt\")[\"pixel_values\"][0]  # tensor (3,224,224)\n",
    "        return px.numpy()\n",
    "\n",
    "    # batched call from datasets.formatting may pass lists\n",
    "    if isinstance(example, dict) and isinstance(example.get(\"image\"), (list, tuple)):\n",
    "        imgs = [proc_one(im) for im in example[\"image\"]]\n",
    "        labs = list(example[\"label\"])\n",
    "        return {\"pixel_values\": imgs, \"label\": labs}\n",
    "    # single example\n",
    "    img = example[\"image\"]\n",
    "    px_arr = proc_one(img)\n",
    "    return {\"pixel_values\": px_arr, \"label\": example[\"label\"]}\n",
    "\n",
    "cub_bird_test_dts = cub_bird_test_raw.with_transform(cub_transform)\n",
    "\n",
    "# 強韌的 collate：把所有樣本疊成 (B,3,224,224)，label 成 (B,)\n",
    "def _to_chw224(x: torch.Tensor) -> torch.Tensor:\n",
    "    x = torch.as_tensor(x)\n",
    "    if not torch.is_floating_point(x):\n",
    "        x = x.float() / 255.0\n",
    "    if x.ndim == 3:\n",
    "        # HWC -> CHW\n",
    "        if x.shape[-1] == 3 and x.shape[0] != 3:\n",
    "            x = x.permute(2, 0, 1)\n",
    "        # 灰階擴通道\n",
    "        if x.shape[0] == 1:\n",
    "            x = x.repeat(3, 1, 1)\n",
    "        elif x.shape[0] != 3:\n",
    "            raise ValueError(f\"Unexpected channel dim: {x.shape}\")\n",
    "    elif x.ndim == 2:\n",
    "        x = x.unsqueeze(0).repeat(3, 1, 1)\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected ndim {x.ndim} for image with shape {tuple(x.shape)}\")\n",
    "    if x.shape[1:] != (224, 224):\n",
    "        x = F.interpolate(x.unsqueeze(0).float(), size=(224, 224),\n",
    "                          mode=\"bilinear\", align_corners=False).squeeze(0)\n",
    "    return x.float()\n",
    "\n",
    "def hf_collate_fn(batch):\n",
    "    imgs = torch.stack([_to_chw224(b[\"pixel_values\"]) for b in batch], dim=0)  # (B,3,224,224)\n",
    "    labs = torch.tensor([int(b[\"label\"]) for b in batch], dtype=torch.long)    # (B,)\n",
    "    return {\"pixel_values\": imgs, \"label\": labs}\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "cub_bird_test_loader = DataLoader(\n",
    "    cub_bird_test_dts,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    collate_fn=hf_collate_fn\n",
    ")\n",
    "\n",
    "# 類別名稱由 HF features 提供\n",
    "cub_bird_class_names = cub_bird_test_raw.features[\"label\"].names\n",
    "\n",
    "import re\n",
    "def clean_cub_name(name: str) -> str:\n",
    "    name = re.sub(r'^\\d+\\.', '', name)   # remove leading numeric prefix\n",
    "    name = name.replace('_', ' ')\n",
    "    return name.strip()\n",
    "\n",
    "cub_bird_class_names = [clean_cub_name(n) for n in cub_bird_class_names]\n",
    "for x in cub_bird_class_names[:10]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d909324e",
   "metadata": {},
   "source": [
    "Start Linear Probing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e8d754",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Starting Method: Linear Probing ---\")\n",
    "\n",
    "# === 1. Load CLIP Vision Model (no text part) ===\n",
    "model = ...\n",
    "\n",
    "# === 2. Freeze backbone ===\n",
    "for p in vision_model.parameters():\n",
    "    p.requires_grad = False\n",
    "for p in visual_projection.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# === 3. Classifier head ===\n",
    "head = # ...\n",
    "    \n",
    "# === 4. Training setup ===\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(object, lr=lr)\n",
    "train_losses, val_losses, val_accuracies = [], [], []\n",
    "\n",
    "# === 5. Training Loop ===\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    head.train()\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \"\"\"\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Train]\"):\n",
    "        pass\n",
    "        \"\"\"\n",
    "        ...\n",
    "        \"\"\"\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} - Train Loss: {train_losses[-1]:.4f} | \"\n",
    "          f\"Val Loss: {val_losses[-1]:.4f} | Val Acc: {val_accuracies[-1]*100:.2f}% | \"\n",
    "          f\"Time: {epoch_end - epoch_start:.2f} sec\")\n",
    "\n",
    "# === 6. Plot curves ===\n",
    "\n",
    "\n",
    "# === 7. Test ===\n",
    "\n",
    "\n",
    "# === 8. Visualization ===\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9f7832",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Starting Method: LoRA Fine-Tuning ---\")\n",
    "\n",
    "# === 1. Load CLIP Vision Model (no text part) ===\n",
    "model = ...\n",
    "\n",
    "# === 2. LoRA config (Q/V projections) ===\n",
    "lora_config = LoraConfig(\n",
    "    \"...\"\n",
    ")\n",
    "\n",
    "# === 3. Wrap with PEFT ===\n",
    "vision_model_lora = get_peft_model(vision_model, lora_config)\n",
    "print(\"LoRA Model - Trainable Parameters:\")\n",
    "vision_model_lora.print_trainable_parameters()\n",
    "\n",
    "# === 4. Freeze projection ===\n",
    "for p in visual_projection.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# === 5. Training setup ===\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(object, lr=lr)\n",
    "train_losses, val_losses, val_accuracies = [], [], []\n",
    "\n",
    "# === 5. Training Loop ===\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    head.train()\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \"\"\"\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Train]\"):\n",
    "        pass\n",
    "        \"\"\"\n",
    "        ...\n",
    "        \"\"\"\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} - Train Loss: {train_losses[-1]:.4f} | \"\n",
    "          f\"Val Loss: {val_losses[-1]:.4f} | Val Acc: {val_accuracies[-1]*100:.2f}% | \"\n",
    "          f\"Time: {epoch_end - epoch_start:.2f} sec\")\n",
    "\n",
    "# === 6. Plot curves ===\n",
    "\n",
    "\n",
    "# === 7. Test ===\n",
    "\n",
    "\n",
    "# === 8. Visualization ===\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
