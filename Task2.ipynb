{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81afeadc",
   "metadata": {},
   "source": [
    "## Task 2 - CLIP Fine-Tuning on the Visual Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8eb2e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python : 3.10.19 | packaged by conda-forge | (main, Oct 22 2025, 22:29:10) [GCC 14.3.0]\n",
      "CUDA   : 12.1\n",
      "Torch  : 2.3.1+cu121\n",
      "Device : NVIDIA GeForce RTX 4090\n",
      "Tue Nov 11 02:51:38 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        Off |   00000000:01:00.0 Off |                  Off |\n",
      "|  0%   49C    P8             20W /  450W |   12345MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      5133      G   /usr/lib/xorg/Xorg                             74MiB |\n",
      "|    0   N/A  N/A      5381      G   /usr/bin/gnome-shell                           63MiB |\n",
      "|    0   N/A  N/A    281992      C   ...nda3/envs/Multimodal_hw2/bin/python      12188MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "Tue Nov 11 02:51:38 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        Off |   00000000:01:00.0 Off |                  Off |\n",
      "|  0%   49C    P8             20W /  450W |   12345MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      5133      G   /usr/lib/xorg/Xorg                             74MiB |\n",
      "|    0   N/A  N/A      5381      G   /usr/bin/gnome-shell                           63MiB |\n",
      "|    0   N/A  N/A    281992      C   ...nda3/envs/Multimodal_hw2/bin/python      12188MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "#@title GPU / Python / Torch sanity\n",
    "import os, sys, subprocess, json, platform, torch\n",
    "print(\"Python :\", sys.version)\n",
    "print(\"CUDA   :\", torch.version.cuda)\n",
    "print(\"Torch  :\", torch.__version__)\n",
    "print(\"Device :\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n",
    "!nvidia-smi || true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dab32194",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abat/conda_envs/clip/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# some imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from transformers import CLIPProcessor, CLIPModel, AutoProcessor, CLIPVisionModel, logging\n",
    "import clip\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from torchinfo import summary\n",
    "from tqdm.autonotebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import json\n",
    "import warnings\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "import io\n",
    "from torchvision.datasets import Flowers102\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def save_training_artifacts(prefix: str, title: str, train_losses, train_accuracies, val_losses=None, val_accuracies=None):\n",
    "    if not train_losses:\n",
    "        return\n",
    "    epochs = list(range(1, len(train_losses) + 1))\n",
    "    data = {\n",
    "        \"epoch\": epochs,\n",
    "        \"train_loss\": train_losses,\n",
    "        \"train_accuracy\": train_accuracies,\n",
    "    }\n",
    "    if val_losses is not None and len(val_losses) == len(epochs):\n",
    "        data[\"val_loss\"] = val_losses\n",
    "    if val_accuracies is not None and len(val_accuracies) == len(epochs):\n",
    "        data[\"val_accuracy\"] = val_accuracies\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    csv_path = os.path.join(\"data\", f\"{prefix}.csv\")\n",
    "    pd.DataFrame(data).to_csv(csv_path, index=False)\n",
    "    fig = None\n",
    "    try:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "        axes[0].plot(epochs, train_losses, label=\"train\")\n",
    "        if val_losses is not None and len(val_losses) == len(epochs):\n",
    "            axes[0].plot(epochs, val_losses, label=\"val\")\n",
    "        axes[0].set_title(\"Loss\")\n",
    "        axes[0].set_xlabel(\"Epoch\")\n",
    "        axes[0].set_ylabel(\"Loss\")\n",
    "        axes[0].legend()\n",
    "\n",
    "        axes[1].plot(epochs, train_accuracies, label=\"train\")\n",
    "        if val_accuracies is not None and len(val_accuracies) == len(epochs):\n",
    "            axes[1].plot(epochs, val_accuracies, label=\"val\")\n",
    "        axes[1].set_title(\"Accuracy\")\n",
    "        axes[1].set_xlabel(\"Epoch\")\n",
    "        axes[1].set_ylabel(\"Accuracy\")\n",
    "        axes[1].legend()\n",
    "\n",
    "        fig.suptitle(title)\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(os.path.join(\"data\", f\"{prefix}.png\"))\n",
    "        plt.show()\n",
    "    except Exception:\n",
    "        pass\n",
    "    finally:\n",
    "        if fig is not None:\n",
    "            plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d0a8323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abat/conda_envs/clip/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 768)\n",
       "      (position_embedding): Embedding(77, 768)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "      (position_embedding): Embedding(257, 1024)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=1024, out_features=768, bias=False)\n",
       "  (text_projection): Linear(in_features=768, out_features=768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 降噪：避免 tokenizers 在多工情境下噴警告\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# some settings\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_ID = \"openai/clip-vit-large-patch14\" # pre-trained CLIP model (ViT-L/14)\n",
    "BATCH_SIZE = 64 # adjust based on your GPU memory\n",
    "NUM_WORKERS = 2\n",
    "gradient_accumulation_steps = 1 # adjust based on your GPU memory\n",
    "# For Linear Probe & LoRA\n",
    "NUM_EPOCHS = 1\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "processor = CLIPProcessor.from_pretrained(MODEL_ID)\n",
    "model     = CLIPModel.from_pretrained(MODEL_ID).to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53344f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test samples (Flowers102): 6149\n"
     ]
    }
   ],
   "source": [
    "# ==== Flowers102 (torchvision) ====\n",
    "\n",
    "\n",
    "# 把 PIL 影像轉成 (3,224,224) 的 CLIP 規格 tensor\n",
    "def clip_image_transform(image):\n",
    "    if not isinstance(image, Image.Image):\n",
    "        image = Image.fromarray(image)\n",
    "    image = image.convert(\"RGB\")\n",
    "    px = processor(images=image, return_tensors=\"pt\")[\"pixel_values\"][0]  # (3,224,224)\n",
    "    return px\n",
    "\n",
    "flowers102_test_dts = Flowers102(\n",
    "    root=\"./data\",\n",
    "    split=\"test\",\n",
    "    transform=clip_image_transform,\n",
    "    download=True\n",
    ")\n",
    "flowers102_train_dts = Flowers102(\n",
    "    root=\"./data\",\n",
    "    split=\"train\",\n",
    "    transform=clip_image_transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "# validation split for Flowers102 (torchvision provides 'val' split)\n",
    "flowers102_val_dts = Flowers102(\n",
    "    root=\"./data\",\n",
    "    split=\"val\",\n",
    "    transform=clip_image_transform,\n",
    "    download=True\n",
    ")\n",
    "print(f\"Total test samples (Flowers102): {len(flowers102_test_dts)}\")  # 6149\n",
    "\n",
    "flowers102_train_loader = DataLoader(\n",
    "    flowers102_train_dts,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "flowers102_val_loader = DataLoader(\n",
    "    flowers102_val_dts,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "flowers102_test_loader = DataLoader(\n",
    "    flowers102_test_dts,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "# 類別名稱（使用你上傳的 cat_to_name.json）\n",
    "with open(\"./data/cat_to_name.json\", \"r\") as f:\n",
    "    cat_to_name = json.load(f)\n",
    "flowers102_class_names = [cat_to_name[str(i)] for i in range(1, 103)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6365bb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test samples (CUB): 5794\n",
      "Black footed Albatross\n",
      "Laysan Albatross\n",
      "Sooty Albatross\n",
      "Groove billed Ani\n",
      "Crested Auklet\n",
      "Least Auklet\n",
      "Parakeet Auklet\n",
      "Rhinoceros Auklet\n",
      "Brewer Blackbird\n",
      "Red winged Blackbird\n"
     ]
    }
   ],
   "source": [
    "# ==== CUB-200-2011 (HF datasets) ====\n",
    "\n",
    "birds_200 = load_dataset(\"bentrevett/caltech-ucsd-birds-200-2011\", cache_dir=\"./data\")\n",
    "cub_bird_test_raw = birds_200[\"test\"]\n",
    "cub_bird_train_raw = birds_200[\"train\"]\n",
    "# build a small validation split from training set (10%)\n",
    "cub_bird_train_raw = cub_bird_train_raw\n",
    "cub_bird_train_raw, cub_bird_val_raw = cub_bird_train_raw.train_test_split(test_size=0.1, seed=42).values()\n",
    "print(f\"Total train/val/test samples (CUB): {len(cub_bird_train_raw)}/{len(cub_bird_val_raw)}/{len(cub_bird_test_raw)}\")\n",
    "\n",
    "def _to_pil_image_safe(img):\n",
    "    \"\"\"Robust convert HF dataset image (PIL/ndarray/list/bytes) -> PIL.Image.\"\"\"\n",
    "    if isinstance(img, Image.Image):\n",
    "        return img\n",
    "    if isinstance(img, np.ndarray):\n",
    "        return Image.fromarray(img)\n",
    "    # list may be list-of-ints (bytes) or nested lists (HWC)\n",
    "    if isinstance(img, (list, tuple)):\n",
    "        # list of ints -> bytes\n",
    "        if all(isinstance(x, (int, np.integer)) for x in img):\n",
    "            return Image.open(io.BytesIO(bytes(img)))\n",
    "        # else try to convert to array (may raise ValueError for ragged)\n",
    "        try:\n",
    "            arr = np.asarray(img, dtype=np.uint8)\n",
    "            return Image.fromarray(arr)\n",
    "        except Exception as ex:\n",
    "            raise TypeError(f\"Cannot convert list image to PIL (ragged?): {ex}\") from ex\n",
    "    if isinstance(img, (bytes, bytearray)):\n",
    "        return Image.open(io.BytesIO(img))\n",
    "    raise TypeError(f\"Unsupported image type for conversion to PIL: {type(img)}\")\n",
    "\n",
    "def cub_transform(example):\n",
    "    \"\"\"\n",
    "    Accept both single example and batch-dict (where example['image'] is a list).\n",
    "    Return pixel_values as numpy arrays (C,H,W) to keep HF formatting/collate stable.\n",
    "    \"\"\"\n",
    "    def proc_one(img):\n",
    "        pil = _to_pil_image_safe(img).convert(\"RGB\")\n",
    "        px = processor(images=pil, return_tensors=\"pt\")[\"pixel_values\"][0]  # tensor (3,224,224)\n",
    "        return px.numpy()\n",
    "\n",
    "    # batched call from datasets.formatting may pass lists\n",
    "    if isinstance(example, dict) and isinstance(example.get(\"image\"), (list, tuple)):\n",
    "        imgs = [proc_one(im) for im in example[\"image\"]]\n",
    "        labs = list(example[\"label\"])\n",
    "        return {\"pixel_values\": imgs, \"label\": labs}\n",
    "    # single example\n",
    "    img = example[\"image\"]\n",
    "    px_arr = proc_one(img)\n",
    "    return {\"pixel_values\": px_arr, \"label\": example[\"label\"]}\n",
    "\n",
    "cub_bird_train_dts = cub_bird_train_raw.with_transform(cub_transform)\n",
    "cub_bird_val_dts = cub_bird_val_raw.with_transform(cub_transform)\n",
    "cub_bird_test_dts = cub_bird_test_raw.with_transform(cub_transform)\n",
    "\n",
    "# 強韌的 collate：把所有樣本疊成 (B,3,224,224)，label 成 (B,)\n",
    "def _to_chw224(x: torch.Tensor) -> torch.Tensor:\n",
    "    x = torch.as_tensor(x)\n",
    "    if not torch.is_floating_point(x):\n",
    "        x = x.float() / 255.0\n",
    "    if x.ndim == 3:\n",
    "        # HWC -> CHW\n",
    "        if x.shape[-1] == 3 and x.shape[0] != 3:\n",
    "            x = x.permute(2, 0, 1)\n",
    "        # 灰階擴通道\n",
    "        if x.shape[0] == 1:\n",
    "            x = x.repeat(3, 1, 1)\n",
    "        elif x.shape[0] != 3:\n",
    "            raise ValueError(f\"Unexpected channel dim: {x.shape}\")\n",
    "    elif x.ndim == 2:\n",
    "        x = x.unsqueeze(0).repeat(3, 1, 1)\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected ndim {x.ndim} for image with shape {tuple(x.shape)}\")\n",
    "    if x.shape[1:] != (224, 224):\n",
    "        x = F.interpolate(x.unsqueeze(0).float(), size=(224, 224),\n",
    "                          mode=\"bilinear\", align_corners=False).squeeze(0)\n",
    "    return x.float()\n",
    "\n",
    "def hf_collate_fn(batch):\n",
    "    imgs = torch.stack([_to_chw224(b[\"pixel_values\"]) for b in batch], dim=0)  # (B,3,224,224)\n",
    "    labs = torch.tensor([int(b[\"label\"]) for b in batch], dtype=torch.long)    # (B,)\n",
    "    return {\"pixel_values\": imgs, \"label\": labs}\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "cub_bird_train_loader = DataLoader(\n",
    "    cub_bird_train_dts,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    collate_fn=hf_collate_fn\n",
    ")\n",
    "\n",
    "cub_bird_val_loader = DataLoader(\n",
    "    cub_bird_val_dts,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    collate_fn=hf_collate_fn\n",
    ")\n",
    "\n",
    "cub_bird_test_loader = DataLoader(\n",
    "    cub_bird_test_dts,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    collate_fn=hf_collate_fn\n",
    ")\n",
    "\n",
    "# 類別名稱由 HF features 提供\n",
    "cub_bird_class_names = cub_bird_test_raw.features[\"label\"].names\n",
    "\n",
    "import re\n",
    "def clean_cub_name(name: str) -> str:\n",
    "    name = re.sub(r'^\\d+\\.', '', name)   # remove leading numeric prefix\n",
    "    name = name.replace('_', ' ')\n",
    "    return name.strip()\n",
    "\n",
    "cub_bird_class_names = [clean_cub_name(n) for n in cub_bird_class_names]\n",
    "for x in cub_bird_class_names[:10]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d909324e",
   "metadata": {},
   "source": [
    "Start Linear Probing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e8d754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Method: Linear Probing ---\n",
      "No training loader prepared for the chosen dataset; running evaluation on test set only\n",
      "No training loader prepared for the chosen dataset; running evaluation on test set only\n",
      "Test Loss: 4.6229 | Test Acc: 0.47%\n",
      "Linear probing finished\n",
      "Test Loss: 4.6229 | Test Acc: 0.47%\n",
      "Linear probing finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"--- Starting Method: Linear Probing Flowers ---\")\n",
    "\n",
    "# We'll use the existing CLIP model (loaded earlier) and attach a linear classifier on top of image features.\n",
    "# Get image feature dimension with a dummy forward\n",
    "with torch.no_grad():\n",
    "    dummy_px = processor(images=Image.new(\"RGB\", (224, 224), \"white\"), return_tensors=\"pt\")[\"pixel_values\"].to(DEVICE)\n",
    "    feat = model.get_image_features(pixel_values=dummy_px)\n",
    "    feat_dim = feat.shape[-1]\n",
    "\n",
    "# Freeze full vision encoder to perform linear probing\n",
    "vision_model = model.vision_model\n",
    "visual_projection = getattr(model, 'visual_projection', None)\n",
    "for p in vision_model.parameters():\n",
    "    p.requires_grad = False\n",
    "if visual_projection is not None:\n",
    "    for p in visual_projection.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "# Simple linear head\n",
    "def make_head(num_classes):\n",
    "    return nn.Linear(feat_dim, num_classes).to(DEVICE)\n",
    "\n",
    "num_classes = len(flowers102_class_names)\n",
    "train_loader = flowers102_train_loader if 'flowers102_train_loader' in globals() else None\n",
    "val_loader = flowers102_val_loader if 'flowers102_val_loader' in globals() else None\n",
    "test_loader = flowers102_test_loader if 'flowers102_test_loader' in globals() else None\n",
    "class_names = flowers102_class_names\n",
    "\n",
    "head = make_head(num_classes)\n",
    "\n",
    "# training setup\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 1e-3\n",
    "optimizer = torch.optim.Adam(head.parameters(), lr=lr)\n",
    "train_losses, train_accuracies, val_losses, val_accuracies = [], [], [], []\n",
    "\n",
    "from time import time\n",
    "\n",
    "def evaluate(loader):\n",
    "    head.eval()\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            if isinstance(batch, dict):\n",
    "                imgs, labels = batch['pixel_values'], batch['label']\n",
    "            else:\n",
    "                imgs, labels = batch\n",
    "            if isinstance(imgs, list):\n",
    "                imgs = torch.stack(imgs, dim=0)\n",
    "            imgs = imgs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            feats = model.get_image_features(pixel_values=imgs)\n",
    "            feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "            logits = head(feats)\n",
    "            loss = criterion(logits, labels)\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            loss_sum += loss.item() * labels.size(0)\n",
    "    return loss_sum / total, correct / total\n",
    "\n",
    "# If no training loader is available, skip training and just run evaluation on test set\n",
    "if train_loader is None:\n",
    "    print('No training loader prepared for the chosen dataset; running evaluation on test set only')\n",
    "    test_loss, test_acc = evaluate(test_loader)\n",
    "    print(f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc*100:.2f}%\")\n",
    "else:\n",
    "    # training loop\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        head.train()\n",
    "        epoch_start = time()\n",
    "        running_loss = 0.0\n",
    "        running_correct = 0\n",
    "        seen = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Train]\"):\n",
    "            if isinstance(batch, dict):\n",
    "                imgs, labels = batch['pixel_values'], batch['label']\n",
    "            else:\n",
    "                imgs, labels = batch\n",
    "            if isinstance(imgs, list):\n",
    "                imgs = torch.stack(imgs, dim=0)\n",
    "            imgs = imgs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            feats = model.get_image_features(pixel_values=imgs)\n",
    "            feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "            logits = head(feats)\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "            running_correct += (preds == labels).sum().item()\n",
    "            seen += labels.size(0)\n",
    "\n",
    "        train_loss = running_loss / max(1, seen)\n",
    "        train_acc = running_correct / max(1, seen)\n",
    "        if val_loader is not None:\n",
    "            val_loss, val_acc = evaluate(val_loader)\n",
    "        else:\n",
    "            val_loss, val_acc = evaluate(test_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "        epoch_end = time()\n",
    "        print(f\"Epoch {epoch+1} - Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}% | Val Loss: {val_loss:.4f} | Val Acc: {val_acc*100:.2f}% | Time: {epoch_end-epoch_start:.2f}s\")\n",
    "\n",
    "    save_training_artifacts(\"linear_flowers\", \"Linear Probing Flowers\", train_losses, train_accuracies, val_losses, val_accuracies)\n",
    "\n",
    "    # final test\n",
    "    test_loss, test_acc = evaluate(test_loader)\n",
    "    print(f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc*100:.2f}%\")\n",
    "\n",
    "print('Linear probing Flowers finished')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbc075f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"--- Starting Method: Linear Probing Birds ---\")\n",
    "\n",
    "# We'll use the existing CLIP model (loaded earlier) and attach a linear classifier on top of image features.\n",
    "# Get image feature dimension with a dummy forward\n",
    "with torch.no_grad():\n",
    "    dummy_px = processor(images=Image.new(\"RGB\", (224, 224), \"white\"), return_tensors=\"pt\")[\"pixel_values\"].to(DEVICE)\n",
    "    feat = model.get_image_features(pixel_values=dummy_px)\n",
    "    feat_dim = feat.shape[-1]\n",
    "\n",
    "# Freeze full vision encoder to perform linear probing\n",
    "vision_model = model.vision_model\n",
    "visual_projection = getattr(model, 'visual_projection', None)\n",
    "for p in vision_model.parameters():\n",
    "    p.requires_grad = False\n",
    "if visual_projection is not None:\n",
    "    for p in visual_projection.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "# Simple linear head\n",
    "def make_head(num_classes):\n",
    "    return nn.Linear(feat_dim, num_classes).to(DEVICE)\n",
    "\n",
    "num_classes = len(cub_bird_class_names)\n",
    "# If train/val splits aren't prepared, fall back to test-only evaluation\n",
    "train_loader = cub_bird_train_loader if 'cub_bird_train_loader' in globals() else None\n",
    "val_loader = cub_bird_val_loader if 'cub_bird_val_loader' in globals() else None\n",
    "test_loader = cub_bird_test_loader if 'cub_bird_test_loader' in globals() else None\n",
    "class_names = cub_bird_class_names\n",
    "\n",
    "head = make_head(num_classes)\n",
    "\n",
    "# training setup\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 1e-3\n",
    "optimizer = torch.optim.Adam(head.parameters(), lr=lr)\n",
    "train_losses, train_accuracies, val_losses, val_accuracies = [], [], [], []\n",
    "\n",
    "from time import time\n",
    "\n",
    "def evaluate(loader):\n",
    "    head.eval()\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            if isinstance(batch, dict):\n",
    "                imgs, labels = batch['pixel_values'], batch['label']\n",
    "            else:\n",
    "                imgs, labels = batch\n",
    "            if isinstance(imgs, list):\n",
    "                imgs = torch.stack(imgs, dim=0)\n",
    "            imgs = imgs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            feats = model.get_image_features(pixel_values=imgs)\n",
    "            feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "            logits = head(feats)\n",
    "            loss = criterion(logits, labels)\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            loss_sum += loss.item() * labels.size(0)\n",
    "    return loss_sum / total, correct / total\n",
    "\n",
    "# If no training loader is available, skip training and just run evaluation on test set\n",
    "if train_loader is None:\n",
    "    print('No training loader prepared for the chosen dataset; running evaluation on test set only')\n",
    "    test_loss, test_acc = evaluate(test_loader)\n",
    "    print(f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc*100:.2f}%\")\n",
    "else:\n",
    "    # training loop\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        head.train()\n",
    "        epoch_start = time()\n",
    "        running_loss = 0.0\n",
    "        running_correct = 0\n",
    "        seen = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Train]\"):\n",
    "            if isinstance(batch, dict):\n",
    "                imgs, labels = batch['pixel_values'], batch['label']\n",
    "            else:\n",
    "                imgs, labels = batch\n",
    "            if isinstance(imgs, list):\n",
    "                imgs = torch.stack(imgs, dim=0)\n",
    "            imgs = imgs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            feats = model.get_image_features(pixel_values=imgs)\n",
    "            feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "            logits = head(feats)\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "            running_correct += (preds == labels).sum().item()\n",
    "            seen += labels.size(0)\n",
    "\n",
    "        train_loss = running_loss / max(1, seen)\n",
    "        train_acc = running_correct / max(1, seen)\n",
    "        if val_loader is not None:\n",
    "            val_loss, val_acc = evaluate(val_loader)\n",
    "        else:\n",
    "            val_loss, val_acc = evaluate(test_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "        epoch_end = time()\n",
    "        print(f\"Epoch {epoch+1} - Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}% | Val Loss: {val_loss:.4f} | Val Acc: {val_acc*100:.2f}% | Time: {epoch_end-epoch_start:.2f}s\")\n",
    "\n",
    "    save_training_artifacts(\"linear_birds\", \"Linear Probing Birds\", train_losses, train_accuracies, val_losses, val_accuracies)\n",
    "\n",
    "    # final test\n",
    "    test_loss, test_acc = evaluate(test_loader)\n",
    "    print(f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc*100:.2f}%\")\n",
    "\n",
    "print('Linear probing Birds finished')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9f7832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Method: LoRA Fine-Tuning ---\n",
      "LoRA path failed or PEFT not available; falling back to tuning visual projection + head. Error: IMAGE_CLASSIFICATION\n",
      "LoRA path failed or PEFT not available; falling back to tuning visual projection + head. Error: IMAGE_CLASSIFICATION\n",
      "No training loader available for fallback; exiting fallback\n",
      "LoRA section finished\n",
      "No training loader available for fallback; exiting fallback\n",
      "LoRA section finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"--- Starting Method: LoRA Fine-Tuning Flowers ---\")\n",
    "\n",
    "train_loader = flowers102_train_loader if 'flowers102_train_loader' in globals() else None\n",
    "val_loader = flowers102_val_loader if 'flowers102_val_loader' in globals() else None\n",
    "test_loader = flowers102_test_loader if 'flowers102_test_loader' in globals() else None\n",
    "num_classes = len(flowers102_class_names)\n",
    "\n",
    "# Try to apply LoRA to the vision encoder. If PEFT is unavailable or fails, fall back to tuning the visual projection + head.\n",
    "try:\n",
    "    # load a fresh CLIP model to avoid interference\n",
    "    model_lora = CLIPModel.from_pretrained(MODEL_ID).to(DEVICE)\n",
    "    vision_model = model_lora.vision_model\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.IMAGE_CLASSIFICATION\n",
    "    )\n",
    "\n",
    "    vision_model_lora = get_peft_model(vision_model, lora_config)\n",
    "    model_lora.vision_model = vision_model_lora\n",
    "    print(\"LoRA Model - Trainable Parameters:\")\n",
    "    vision_model_lora.print_trainable_parameters()\n",
    "\n",
    "    # freeze visual projection and text encoder if present\n",
    "    if hasattr(model_lora, 'visual_projection') and model_lora.visual_projection is not None:\n",
    "        for p in model_lora.visual_projection.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    # prepare head\n",
    "    with torch.no_grad():\n",
    "        dummy_px = processor(images=Image.new(\"RGB\", (224, 224), \"white\"), return_tensors=\"pt\")[\"pixel_values\"].to(DEVICE)\n",
    "        feat = model_lora.get_image_features(pixel_values=dummy_px)\n",
    "        feat_dim = feat.shape[-1]\n",
    "    head = nn.Linear(feat_dim, num_classes).to(DEVICE)\n",
    "\n",
    "    params = list(filter(lambda p: p.requires_grad, model_lora.parameters())) + list(head.parameters())\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(params, lr=5e-4)\n",
    "\n",
    "    def eval_model(m):\n",
    "        m.eval(); head.eval()\n",
    "        total, correct, loss_sum = 0, 0, 0.0\n",
    "        loader = val_loader if val_loader is not None else test_loader\n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                if isinstance(batch, dict):\n",
    "                    imgs, labels = batch['pixel_values'], batch['label']\n",
    "                else:\n",
    "                    imgs, labels = batch\n",
    "                if isinstance(imgs, list):\n",
    "                    imgs = torch.stack(imgs, dim=0)\n",
    "                imgs = imgs.to(DEVICE)\n",
    "                labels = labels.to(DEVICE)\n",
    "                feats = m.get_image_features(pixel_values=imgs)\n",
    "                feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "                logits = head(feats)\n",
    "                loss = criterion(logits, labels)\n",
    "                preds = logits.argmax(dim=-1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "                loss_sum += loss.item() * labels.size(0)\n",
    "        return loss_sum/total, correct/total\n",
    "\n",
    "    # train LoRA + head (if train_loader exists)\n",
    "    if train_loader is None:\n",
    "        print('No training loader available for LoRA path; running quick eval instead')\n",
    "        tloss, tacc = eval_model(model_lora)\n",
    "        print(f\"Val/Test Loss: {tloss:.4f} | Acc: {tacc*100:.2f}%\")\n",
    "    else:\n",
    "        train_losses, train_accuracies, val_losses, val_accuracies = [], [], [], []\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            model_lora.train(); head.train()\n",
    "            running_loss = 0.0\n",
    "            running_correct = 0\n",
    "            seen = 0\n",
    "            for batch in tqdm(train_loader, desc=f\"LoRA Epoch {epoch+1}/{NUM_EPOCHS} [Train]\"):\n",
    "                if isinstance(batch, dict):\n",
    "                    imgs, labels = batch['pixel_values'], batch['label']\n",
    "                else:\n",
    "                    imgs, labels = batch\n",
    "                if isinstance(imgs, list):\n",
    "                    imgs = torch.stack(imgs, dim=0)\n",
    "                imgs = imgs.to(DEVICE)\n",
    "                labels = labels.to(DEVICE)\n",
    "                feats = model_lora.get_image_features(pixel_values=imgs)\n",
    "                feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "                logits = head(feats)\n",
    "                preds = logits.argmax(dim=-1)\n",
    "                loss = criterion(logits, labels)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * labels.size(0)\n",
    "                running_correct += (preds == labels).sum().item()\n",
    "                seen += labels.size(0)\n",
    "            train_loss = running_loss / max(1, seen)\n",
    "            train_acc = running_correct / max(1, seen)\n",
    "            val_loss, val_acc = eval_model(model_lora)\n",
    "            train_losses.append(train_loss)\n",
    "            train_accuracies.append(train_acc)\n",
    "            val_losses.append(val_loss)\n",
    "            val_accuracies.append(val_acc)\n",
    "            print(f\"LoRA Epoch {epoch+1} - Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}% | Val Loss: {val_loss:.4f} | Val Acc: {val_acc*100:.2f}%\")\n",
    "        save_training_artifacts(\"finetuning_flowers\", \"LoRA Fine-Tuning Flowers\", train_losses, train_accuracies, val_losses, val_accuracies)\n",
    "        test_loss, test_acc = eval_model(model_lora)\n",
    "        print(f\"LoRA Test Loss: {test_loss:.4f} | Test Acc: {test_acc*100:.2f}%\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"LoRA path failed or PEFT not available; falling back to tuning visual projection + head. Error:\", e)\n",
    "    # fallback approach\n",
    "    model_fb = CLIPModel.from_pretrained(MODEL_ID).to(DEVICE)\n",
    "    if hasattr(model_fb, 'visual_projection') and model_fb.visual_projection is not None:\n",
    "        for p in model_fb.visual_projection.parameters():\n",
    "            p.requires_grad = True\n",
    "    for p in model_fb.vision_model.parameters():\n",
    "        p.requires_grad = False\n",
    "    with torch.no_grad():\n",
    "        dummy_px = processor(images=Image.new(\"RGB\", (224, 224), \"white\"), return_tensors=\"pt\")[\"pixel_values\"].to(DEVICE)\n",
    "        feat = model_fb.get_image_features(pixel_values=dummy_px)\n",
    "        feat_dim = feat.shape[-1]\n",
    "    head = nn.Linear(feat_dim, num_classes).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(list(head.parameters()) + list(filter(lambda p: p.requires_grad, model_fb.parameters())), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if train_loader is None:\n",
    "        print('No training loader available for fallback; exiting fallback')\n",
    "    else:\n",
    "        fallback_train_losses, fallback_train_accuracies = [], []\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            model_fb.train(); head.train()\n",
    "            running_loss = 0.0\n",
    "            running_correct = 0\n",
    "            seen = 0\n",
    "            for batch in tqdm(train_loader, desc=f\"Fallback Epoch {epoch+1}/{NUM_EPOCHS} [Train]\"):\n",
    "                if isinstance(batch, dict):\n",
    "                    imgs, labels = batch['pixel_values'], batch['label']\n",
    "                else:\n",
    "                    imgs, labels = batch\n",
    "                if isinstance(imgs, list):\n",
    "                    imgs = torch.stack(imgs, dim=0)\n",
    "                imgs = imgs.to(DEVICE); labels = labels.to(DEVICE)\n",
    "                feats = model_fb.get_image_features(pixel_values=imgs)\n",
    "                feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "                logits = head(feats)\n",
    "                preds = logits.argmax(dim=-1)\n",
    "                loss = criterion(logits, labels)\n",
    "                optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "                running_loss += loss.item()*labels.size(0)\n",
    "                running_correct += (preds == labels).sum().item()\n",
    "                seen += labels.size(0)\n",
    "            train_loss = running_loss / max(1, seen)\n",
    "            train_acc = running_correct / max(1, seen)\n",
    "            fallback_train_losses.append(train_loss)\n",
    "            fallback_train_accuracies.append(train_acc)\n",
    "            print(f\"Fallback epoch {epoch+1} - Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "        save_training_artifacts(\"finetuning_flowers\", \"Fallback Fine-Tuning Flowers\", fallback_train_losses, fallback_train_accuracies)\n",
    "        # Evaluate fallback\n",
    "        def eval_fb():\n",
    "            head.eval(); model_fb.eval()\n",
    "            total, correct = 0, 0\n",
    "            loader = val_loader if val_loader is not None else test_loader\n",
    "            with torch.no_grad():\n",
    "                for batch in loader:\n",
    "                    if isinstance(batch, dict):\n",
    "                        imgs, labels = batch['pixel_values'], batch['label']\n",
    "                    else:\n",
    "                        imgs, labels = batch\n",
    "                    if isinstance(imgs, list):\n",
    "                        imgs = torch.stack(imgs, dim=0)\n",
    "                    imgs = imgs.to(DEVICE); labels = labels.to(DEVICE)\n",
    "                    feats = model_fb.get_image_features(pixel_values=imgs)\n",
    "                    feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "                    preds = head(feats).argmax(dim=-1)\n",
    "                    correct += (preds == labels).sum().item(); total += labels.size(0)\n",
    "            return correct/total\n",
    "        print(f\"Fallback Val Acc: {eval_fb()*100:.2f}%\")\n",
    "\n",
    "print('LoRA Flowers finished')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718945c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"--- Starting Method: LoRA Fine-Tuning Birds ---\")\n",
    "\n",
    "train_loader = cub_bird_train_loader if 'cub_bird_train_loader' in globals() else None\n",
    "val_loader = cub_bird_val_loader if 'cub_bird_val_loader' in globals() else None\n",
    "test_loader = cub_bird_test_loader if 'cub_bird_test_loader' in globals() else None\n",
    "num_classes = len(cub_bird_class_names)\n",
    "\n",
    "# Try to apply LoRA to the vision encoder. If PEFT is unavailable or fails, fall back to tuning the visual projection + head.\n",
    "try:\n",
    "    # load a fresh CLIP model to avoid interference\n",
    "    model_lora = CLIPModel.from_pretrained(MODEL_ID).to(DEVICE)\n",
    "    vision_model = model_lora.vision_model\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.IMAGE_CLASSIFICATION\n",
    "    )\n",
    "\n",
    "    vision_model_lora = get_peft_model(vision_model, lora_config)\n",
    "    model_lora.vision_model = vision_model_lora\n",
    "    print(\"LoRA Model - Trainable Parameters:\")\n",
    "    vision_model_lora.print_trainable_parameters()\n",
    "\n",
    "    # freeze visual projection and text encoder if present\n",
    "    if hasattr(model_lora, 'visual_projection') and model_lora.visual_projection is not None:\n",
    "        for p in model_lora.visual_projection.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    # prepare head\n",
    "    with torch.no_grad():\n",
    "        dummy_px = processor(images=Image.new(\"RGB\", (224, 224), \"white\"), return_tensors=\"pt\")[\"pixel_values\"].to(DEVICE)\n",
    "        feat = model_lora.get_image_features(pixel_values=dummy_px)\n",
    "        feat_dim = feat.shape[-1]\n",
    "    head = nn.Linear(feat_dim, num_classes).to(DEVICE)\n",
    "\n",
    "    params = list(filter(lambda p: p.requires_grad, model_lora.parameters())) + list(head.parameters())\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(params, lr=5e-4)\n",
    "\n",
    "    def eval_model(m):\n",
    "        m.eval(); head.eval()\n",
    "        total, correct, loss_sum = 0, 0, 0.0\n",
    "        loader = val_loader if val_loader is not None else test_loader\n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                if isinstance(batch, dict):\n",
    "                    imgs, labels = batch['pixel_values'], batch['label']\n",
    "                else:\n",
    "                    imgs, labels = batch\n",
    "                if isinstance(imgs, list):\n",
    "                    imgs = torch.stack(imgs, dim=0)\n",
    "                imgs = imgs.to(DEVICE)\n",
    "                labels = labels.to(DEVICE)\n",
    "                feats = m.get_image_features(pixel_values=imgs)\n",
    "                feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "                logits = head(feats)\n",
    "                loss = criterion(logits, labels)\n",
    "                preds = logits.argmax(dim=-1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "                loss_sum += loss.item() * labels.size(0)\n",
    "        return loss_sum/total, correct/total\n",
    "\n",
    "    # train LoRA + head (if train_loader exists)\n",
    "    if train_loader is None:\n",
    "        print('No training loader available for LoRA path; running quick eval instead')\n",
    "        tloss, tacc = eval_model(model_lora)\n",
    "        print(f\"Val/Test Loss: {tloss:.4f} | Acc: {tacc*100:.2f}%\")\n",
    "    else:\n",
    "        train_losses, train_accuracies, val_losses, val_accuracies = [], [], [], []\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            model_lora.train(); head.train()\n",
    "            running_loss = 0.0\n",
    "            running_correct = 0\n",
    "            seen = 0\n",
    "            for batch in tqdm(train_loader, desc=f\"LoRA Epoch {epoch+1}/{NUM_EPOCHS} [Train]\"):\n",
    "                if isinstance(batch, dict):\n",
    "                    imgs, labels = batch['pixel_values'], batch['label']\n",
    "                else:\n",
    "                    imgs, labels = batch\n",
    "                if isinstance(imgs, list):\n",
    "                    imgs = torch.stack(imgs, dim=0)\n",
    "                imgs = imgs.to(DEVICE)\n",
    "                labels = labels.to(DEVICE)\n",
    "                feats = model_lora.get_image_features(pixel_values=imgs)\n",
    "                feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "                logits = head(feats)\n",
    "                preds = logits.argmax(dim=-1)\n",
    "                loss = criterion(logits, labels)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * labels.size(0)\n",
    "                running_correct += (preds == labels).sum().item()\n",
    "                seen += labels.size(0)\n",
    "            train_loss = running_loss / max(1, seen)\n",
    "            train_acc = running_correct / max(1, seen)\n",
    "            val_loss, val_acc = eval_model(model_lora)\n",
    "            train_losses.append(train_loss)\n",
    "            train_accuracies.append(train_acc)\n",
    "            val_losses.append(val_loss)\n",
    "            val_accuracies.append(val_acc)\n",
    "            print(f\"LoRA Epoch {epoch+1} - Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}% | Val Loss: {val_loss:.4f} | Val Acc: {val_acc*100:.2f}%\")\n",
    "        save_training_artifacts(\"finetuning_birds\", \"LoRA Fine-Tuning Birds\", train_losses, train_accuracies, val_losses, val_accuracies)\n",
    "        test_loss, test_acc = eval_model(model_lora)\n",
    "        print(f\"LoRA Test Loss: {test_loss:.4f} | Test Acc: {test_acc*100:.2f}%\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"LoRA path failed or PEFT not available; falling back to tuning visual projection + head. Error:\", e)\n",
    "    # fallback approach\n",
    "    model_fb = CLIPModel.from_pretrained(MODEL_ID).to(DEVICE)\n",
    "    if hasattr(model_fb, 'visual_projection') and model_fb.visual_projection is not None:\n",
    "        for p in model_fb.visual_projection.parameters():\n",
    "            p.requires_grad = True\n",
    "    for p in model_fb.vision_model.parameters():\n",
    "        p.requires_grad = False\n",
    "    with torch.no_grad():\n",
    "        dummy_px = processor(images=Image.new(\"RGB\", (224, 224), \"white\"), return_tensors=\"pt\")[\"pixel_values\"].to(DEVICE)\n",
    "        feat = model_fb.get_image_features(pixel_values=dummy_px)\n",
    "        feat_dim = feat.shape[-1]\n",
    "    head = nn.Linear(feat_dim, num_classes).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(list(head.parameters()) + list(filter(lambda p: p.requires_grad, model_fb.parameters())), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if train_loader is None:\n",
    "        print('No training loader available for fallback; exiting fallback')\n",
    "    else:\n",
    "        fallback_train_losses, fallback_train_accuracies = [], []\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            model_fb.train(); head.train()\n",
    "            running_loss = 0.0\n",
    "            running_correct = 0\n",
    "            seen = 0\n",
    "            for batch in tqdm(train_loader, desc=f\"Fallback Epoch {epoch+1}/{NUM_EPOCHS} [Train]\"):\n",
    "                if isinstance(batch, dict):\n",
    "                    imgs, labels = batch['pixel_values'], batch['label']\n",
    "                else:\n",
    "                    imgs, labels = batch\n",
    "                if isinstance(imgs, list):\n",
    "                    imgs = torch.stack(imgs, dim=0)\n",
    "                imgs = imgs.to(DEVICE); labels = labels.to(DEVICE)\n",
    "                feats = model_fb.get_image_features(pixel_values=imgs)\n",
    "                feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "                logits = head(feats)\n",
    "                preds = logits.argmax(dim=-1)\n",
    "                loss = criterion(logits, labels)\n",
    "                optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "                running_loss += loss.item()*labels.size(0)\n",
    "                running_correct += (preds == labels).sum().item()\n",
    "                seen += labels.size(0)\n",
    "            train_loss = running_loss / max(1, seen)\n",
    "            train_acc = running_correct / max(1, seen)\n",
    "            fallback_train_losses.append(train_loss)\n",
    "            fallback_train_accuracies.append(train_acc)\n",
    "            print(f\"Fallback epoch {epoch+1} - Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "        save_training_artifacts(\"finetuning_birds\", \"Fallback Fine-Tuning Birds\", fallback_train_losses, fallback_train_accuracies)\n",
    "        # Evaluate fallback\n",
    "        def eval_fb():\n",
    "            head.eval(); model_fb.eval()\n",
    "            total, correct = 0, 0\n",
    "            loader = val_loader if val_loader is not None else test_loader\n",
    "            with torch.no_grad():\n",
    "                for batch in loader:\n",
    "                    if isinstance(batch, dict):\n",
    "                        imgs, labels = batch['pixel_values'], batch['label']\n",
    "                    else:\n",
    "                        imgs, labels = batch\n",
    "                    if isinstance(imgs, list):\n",
    "                        imgs = torch.stack(imgs, dim=0)\n",
    "                    imgs = imgs.to(DEVICE); labels = labels.to(DEVICE)\n",
    "                    feats = model_fb.get_image_features(pixel_values=imgs)\n",
    "                    feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "                    preds = head(feats).argmax(dim=-1)\n",
    "                    correct += (preds == labels).sum().item(); total += labels.size(0)\n",
    "            return correct/total\n",
    "        print(f\"Fallback Val Acc: {eval_fb()*100:.2f}%\")\n",
    "\n",
    "print('LoRA Birds finished')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
