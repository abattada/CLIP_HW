{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81afeadc",
   "metadata": {},
   "source": [
    "## Task 2 - CLIP Fine-Tuning on the Visual Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8eb2e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python : 3.10.19 | packaged by conda-forge | (main, Oct 22 2025, 22:29:10) [GCC 14.3.0]\n",
      "CUDA   : 12.1\n",
      "Torch  : 2.3.1+cu121\n",
      "Device : NVIDIA GeForce RTX 4090\n",
      "Tue Nov 11 19:07:02 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        Off |   00000000:01:00.0 Off |                  Off |\n",
      "| 35%   49C    P3             39W /  450W |     152MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      5133      G   /usr/lib/xorg/Xorg                             75MiB |\n",
      "|    0   N/A  N/A      5381      G   /usr/bin/gnome-shell                           63MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "Tue Nov 11 19:07:02 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        Off |   00000000:01:00.0 Off |                  Off |\n",
      "| 35%   49C    P3             39W /  450W |     152MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      5133      G   /usr/lib/xorg/Xorg                             75MiB |\n",
      "|    0   N/A  N/A      5381      G   /usr/bin/gnome-shell                           63MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "#@title GPU / Python / Torch sanity\n",
    "import os, sys, subprocess, json, platform, torch\n",
    "print(\"Python :\", sys.version)\n",
    "print(\"CUDA   :\", torch.version.cuda)\n",
    "print(\"Torch  :\", torch.__version__)\n",
    "print(\"Device :\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n",
    "!nvidia-smi || true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dab32194",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abat/conda_envs/clip/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# some imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from transformers import CLIPProcessor, CLIPModel, AutoProcessor, CLIPVisionModel, logging\n",
    "import clip\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from torchinfo import summary\n",
    "from tqdm.autonotebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import json\n",
    "import warnings\n",
    "import gc, torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "import io\n",
    "from torchvision.datasets import Flowers102\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def save_training_artifacts(prefix: str, title: str, train_losses, train_accuracies, val_losses=None, val_accuracies=None):\n",
    "    if not train_losses:\n",
    "        return\n",
    "    epochs = list(range(1, len(train_losses) + 1))\n",
    "    data = {\n",
    "        \"epoch\": epochs,\n",
    "        \"train_loss\": train_losses,\n",
    "        \"train_accuracy\": train_accuracies,\n",
    "    }\n",
    "    if val_losses is not None and len(val_losses) == len(epochs):\n",
    "        data[\"val_loss\"] = val_losses\n",
    "    if val_accuracies is not None and len(val_accuracies) == len(epochs):\n",
    "        data[\"val_accuracy\"] = val_accuracies\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    csv_path = os.path.join(\"data\", f\"{prefix}.csv\")\n",
    "    pd.DataFrame(data).to_csv(csv_path, index=False)\n",
    "    fig = None\n",
    "    try:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "        axes[0].plot(epochs, train_losses, label=\"train\")\n",
    "        if val_losses is not None and len(val_losses) == len(epochs):\n",
    "            axes[0].plot(epochs, val_losses, label=\"val\")\n",
    "        axes[0].set_title(\"Loss\")\n",
    "        axes[0].set_xlabel(\"Epoch\")\n",
    "        axes[0].set_ylabel(\"Loss\")\n",
    "        axes[0].legend()\n",
    "\n",
    "        axes[1].plot(epochs, train_accuracies, label=\"train\")\n",
    "        if val_accuracies is not None and len(val_accuracies) == len(epochs):\n",
    "            axes[1].plot(epochs, val_accuracies, label=\"val\")\n",
    "        axes[1].set_title(\"Accuracy\")\n",
    "        axes[1].set_xlabel(\"Epoch\")\n",
    "        axes[1].set_ylabel(\"Accuracy\")\n",
    "        axes[1].legend()\n",
    "\n",
    "        fig.suptitle(title)\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(os.path.join(\"data\", f\"{prefix}.png\"))\n",
    "        plt.show()\n",
    "    except Exception:\n",
    "        pass\n",
    "    finally:\n",
    "        if fig is not None:\n",
    "            plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d0a8323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abat/conda_envs/clip/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 768)\n",
       "      (position_embedding): Embedding(77, 768)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "      (position_embedding): Embedding(257, 1024)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=1024, out_features=768, bias=False)\n",
       "  (text_projection): Linear(in_features=768, out_features=768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 降噪：避免 tokenizers 在多工情境下噴警告\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# some settings\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_ID = \"openai/clip-vit-large-patch14\" # pre-trained CLIP model (ViT-L/14)\n",
    "LINEAR_BATCH_SIZE = 1024 # adjust based on your GPU memory\n",
    "FINE_BATCH_SIZE   = 16  # adjust based on your GPU memory\n",
    "NUM_WORKERS = 8\n",
    "# For Linear Probe & LoRA\n",
    "NUM_EPOCHS = 30\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "processor = CLIPProcessor.from_pretrained(MODEL_ID)\n",
    "model     = CLIPModel.from_pretrained(MODEL_ID).to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53344f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test samples (Flowers102): 6149\n"
     ]
    }
   ],
   "source": [
    "# ==== Flowers102 (torchvision) ====\n",
    "\n",
    "\n",
    "# 把 PIL 影像轉成 (3,224,224) 的 CLIP 規格 tensor\n",
    "def clip_image_transform(image):\n",
    "    if not isinstance(image, Image.Image):\n",
    "        image = Image.fromarray(image)\n",
    "    image = image.convert(\"RGB\")\n",
    "    px = processor(images=image, return_tensors=\"pt\")[\"pixel_values\"][0]  # (3,224,224)\n",
    "    return px\n",
    "\n",
    "flowers102_test_dts = Flowers102(\n",
    "    root=\"./data\",\n",
    "    split=\"test\",\n",
    "    transform=clip_image_transform,\n",
    "    download=True\n",
    ")\n",
    "flowers102_train_dts = Flowers102(\n",
    "    root=\"./data\",\n",
    "    split=\"train\",\n",
    "    transform=clip_image_transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "# validation split for Flowers102 (torchvision provides 'val' split)\n",
    "flowers102_val_dts = Flowers102(\n",
    "    root=\"./data\",\n",
    "    split=\"val\",\n",
    "    transform=clip_image_transform,\n",
    "    download=True\n",
    ")\n",
    "print(f\"Total test samples (Flowers102): {len(flowers102_test_dts)}\")  # 6149\n",
    "\n",
    "flowers102_train_loader = DataLoader(\n",
    "    flowers102_train_dts,\n",
    "    batch_size=LINEAR_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "flowers102_val_loader = DataLoader(\n",
    "    flowers102_val_dts,\n",
    "    batch_size=LINEAR_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "flowers102_test_loader = DataLoader(\n",
    "    flowers102_test_dts,\n",
    "    batch_size=LINEAR_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "# 類別名稱（使用你上傳的 cat_to_name.json）\n",
    "with open(\"./data/cat_to_name.json\", \"r\") as f:\n",
    "    cat_to_name = json.load(f)\n",
    "flowers102_class_names = [cat_to_name[str(i)] for i in range(1, 103)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6365bb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train/val/test samples (CUB): 5394/600/5794\n",
      "Black footed Albatross\n",
      "Laysan Albatross\n",
      "Sooty Albatross\n",
      "Groove billed Ani\n",
      "Crested Auklet\n",
      "Least Auklet\n",
      "Parakeet Auklet\n",
      "Rhinoceros Auklet\n",
      "Brewer Blackbird\n",
      "Red winged Blackbird\n"
     ]
    }
   ],
   "source": [
    "# ==== CUB-200-2011 (HF datasets) ====\n",
    "\n",
    "birds_200 = load_dataset(\"bentrevett/caltech-ucsd-birds-200-2011\", cache_dir=\"./data\")\n",
    "cub_bird_test_raw = birds_200[\"test\"]\n",
    "cub_bird_train_raw = birds_200[\"train\"]\n",
    "# build a small validation split from training set (10%)\n",
    "cub_bird_train_raw = cub_bird_train_raw\n",
    "cub_bird_train_raw, cub_bird_val_raw = cub_bird_train_raw.train_test_split(test_size=0.1, seed=42).values()\n",
    "print(f\"Total train/val/test samples (CUB): {len(cub_bird_train_raw)}/{len(cub_bird_val_raw)}/{len(cub_bird_test_raw)}\")\n",
    "\n",
    "def _to_pil_image_safe(img):\n",
    "    \"\"\"Robust convert HF dataset image (PIL/ndarray/list/bytes) -> PIL.Image.\"\"\"\n",
    "    if isinstance(img, Image.Image):\n",
    "        return img\n",
    "    if isinstance(img, np.ndarray):\n",
    "        return Image.fromarray(img)\n",
    "    # list may be list-of-ints (bytes) or nested lists (HWC)\n",
    "    if isinstance(img, (list, tuple)):\n",
    "        # list of ints -> bytes\n",
    "        if all(isinstance(x, (int, np.integer)) for x in img):\n",
    "            return Image.open(io.BytesIO(bytes(img)))\n",
    "        # else try to convert to array (may raise ValueError for ragged)\n",
    "        try:\n",
    "            arr = np.asarray(img, dtype=np.uint8)\n",
    "            return Image.fromarray(arr)\n",
    "        except Exception as ex:\n",
    "            raise TypeError(f\"Cannot convert list image to PIL (ragged?): {ex}\") from ex\n",
    "    if isinstance(img, (bytes, bytearray)):\n",
    "        return Image.open(io.BytesIO(img))\n",
    "    raise TypeError(f\"Unsupported image type for conversion to PIL: {type(img)}\")\n",
    "\n",
    "def cub_transform(example):\n",
    "    \"\"\"\n",
    "    Accept both single example and batch-dict (where example['image'] is a list).\n",
    "    Return pixel_values as numpy arrays (C,H,W) to keep HF formatting/collate stable.\n",
    "    \"\"\"\n",
    "    def proc_one(img):\n",
    "        pil = _to_pil_image_safe(img).convert(\"RGB\")\n",
    "        px = processor(images=pil, return_tensors=\"pt\")[\"pixel_values\"][0]  # tensor (3,224,224)\n",
    "        return px.numpy()\n",
    "\n",
    "    # batched call from datasets.formatting may pass lists\n",
    "    if isinstance(example, dict) and isinstance(example.get(\"image\"), (list, tuple)):\n",
    "        imgs = [proc_one(im) for im in example[\"image\"]]\n",
    "        labs = list(example[\"label\"])\n",
    "        return {\"pixel_values\": imgs, \"label\": labs}\n",
    "    # single example\n",
    "    img = example[\"image\"]\n",
    "    px_arr = proc_one(img)\n",
    "    return {\"pixel_values\": px_arr, \"label\": example[\"label\"]}\n",
    "\n",
    "cub_bird_train_dts = cub_bird_train_raw.with_transform(cub_transform)\n",
    "cub_bird_val_dts = cub_bird_val_raw.with_transform(cub_transform)\n",
    "cub_bird_test_dts = cub_bird_test_raw.with_transform(cub_transform)\n",
    "\n",
    "# 強韌的 collate：把所有樣本疊成 (B,3,224,224)，label 成 (B,)\n",
    "def _to_chw224(x: torch.Tensor) -> torch.Tensor:\n",
    "    x = torch.as_tensor(x)\n",
    "    if not torch.is_floating_point(x):\n",
    "        x = x.float() / 255.0\n",
    "    if x.ndim == 3:\n",
    "        # HWC -> CHW\n",
    "        if x.shape[-1] == 3 and x.shape[0] != 3:\n",
    "            x = x.permute(2, 0, 1)\n",
    "        # 灰階擴通道\n",
    "        if x.shape[0] == 1:\n",
    "            x = x.repeat(3, 1, 1)\n",
    "        elif x.shape[0] != 3:\n",
    "            raise ValueError(f\"Unexpected channel dim: {x.shape}\")\n",
    "    elif x.ndim == 2:\n",
    "        x = x.unsqueeze(0).repeat(3, 1, 1)\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected ndim {x.ndim} for image with shape {tuple(x.shape)}\")\n",
    "    if x.shape[1:] != (224, 224):\n",
    "        x = F.interpolate(x.unsqueeze(0).float(), size=(224, 224),\n",
    "                          mode=\"bilinear\", align_corners=False).squeeze(0)\n",
    "    return x.float()\n",
    "\n",
    "def hf_collate_fn(batch):\n",
    "    imgs = torch.stack([_to_chw224(b[\"pixel_values\"]) for b in batch], dim=0)  # (B,3,224,224)\n",
    "    labs = torch.tensor([int(b[\"label\"]) for b in batch], dtype=torch.long)    # (B,)\n",
    "    return {\"pixel_values\": imgs, \"label\": labs}\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "cub_bird_train_loader = DataLoader(\n",
    "    cub_bird_train_dts,\n",
    "    batch_size=LINEAR_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    collate_fn=hf_collate_fn\n",
    ")\n",
    "\n",
    "cub_bird_val_loader = DataLoader(\n",
    "    cub_bird_val_dts,\n",
    "    batch_size=LINEAR_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    collate_fn=hf_collate_fn\n",
    ")\n",
    "\n",
    "cub_bird_test_loader = DataLoader(\n",
    "    cub_bird_test_dts,\n",
    "    batch_size=LINEAR_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    collate_fn=hf_collate_fn\n",
    ")\n",
    "\n",
    "# 類別名稱由 HF features 提供\n",
    "cub_bird_class_names = cub_bird_test_raw.features[\"label\"].names\n",
    "\n",
    "import re\n",
    "def clean_cub_name(name: str) -> str:\n",
    "    name = re.sub(r'^\\d+\\.', '', name)   # remove leading numeric prefix\n",
    "    name = name.replace('_', ' ')\n",
    "    return name.strip()\n",
    "\n",
    "cub_bird_class_names = [clean_cub_name(n) for n in cub_bird_class_names]\n",
    "for x in cub_bird_class_names[:10]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45038c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 工具函式：比較原始 CLIP 與訓練後模型的預測\n",
    "from typing import Any, Dict, List, Optional, Sequence, Tuple\n",
    "import math\n",
    "\n",
    "DATASET_REGISTRY: Dict[str, Dict[str, Any]] = {\n",
    "    \"flowers102\": {\n",
    "        \"splits\": {\n",
    "            \"train\": flowers102_train_dts,\n",
    "            \"val\": flowers102_val_dts,\n",
    "            \"test\": flowers102_test_dts,\n",
    "        },\n",
    "        \"class_names\": flowers102_class_names,\n",
    "    },\n",
    "    \"cub_bird\": {\n",
    "        \"splits\": {\n",
    "            \"train\": cub_bird_train_dts,\n",
    "            \"val\": cub_bird_val_dts,\n",
    "            \"test\": cub_bird_test_dts,\n",
    "        },\n",
    "        \"class_names\": cub_bird_class_names,\n",
    "    },\n",
    "}\n",
    "\n",
    "for entry in DATASET_REGISTRY.values():\n",
    "    entry[\"prompts\"] = [f\"a photo of a {name}\" for name in entry[\"class_names\"]]\n",
    "\n",
    "TRAINED_VARIANTS: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "def save_trained_variant_to_disk(variant_name: str, base_dir: str = \"./saved_variants\"):\n",
    "    \"\"\"保存 TRAINED_VARIANTS[variant_name] 的 backbone 與 head 到磁碟（HF format + head state_dict）。\"\"\"\n",
    "    variant = TRAINED_VARIANTS.get(variant_name)\n",
    "    if variant is None:\n",
    "        raise KeyError(f\"Variant '{variant_name}' not found in TRAINED_VARIANTS.\")\n",
    "    out = Path(base_dir) / variant_name\n",
    "    out.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    backbone = variant[\"backbone\"]\n",
    "    head = variant[\"head\"]\n",
    "\n",
    "    # try HF-style save first\n",
    "    try:\n",
    "        backbone.save_pretrained(str(out / \"backbone\"))\n",
    "    except Exception:\n",
    "        try:\n",
    "            torch.save(backbone.state_dict(), str(out / \"backbone_state.pth\"))\n",
    "        except Exception as e:\n",
    "            print(\"Warning: failed to save backbone with both save_pretrained and state_dict:\", e)\n",
    "\n",
    "    try:\n",
    "        torch.save(head.state_dict(), str(out / \"head_state.pth\"))\n",
    "    except Exception as e:\n",
    "        print(\"Warning: failed to save head state_dict:\", e)\n",
    "\n",
    "    meta = {\n",
    "        \"dataset\": variant.get(\"dataset\"),\n",
    "        \"notes\": variant.get(\"notes\", \"\"),\n",
    "    }\n",
    "    with open(out / \"meta.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Saved variant '{variant_name}' to {out}\")\n",
    "    \n",
    "def load_trained_variant_from_disk(variant_name: str, base_dir: str = \"./saved_variants\", device: str = None):\n",
    "    \"\"\"從磁碟載入 previously-saved variant。回傳 (backbone, head). device 可為 'cpu' 或 'cuda' 等。\"\"\"\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    base = Path(base_dir) / variant_name\n",
    "    if not base.exists():\n",
    "        raise FileNotFoundError(f\"No saved variant at {base}\")\n",
    "\n",
    "    backbone = None\n",
    "    # prefer HF-style dir\n",
    "    hf_dir = base / \"backbone\"\n",
    "    if hf_dir.exists():\n",
    "        backbone = CLIPModel.from_pretrained(str(hf_dir)).to(device)\n",
    "    else:\n",
    "        # try load state_dict into fresh model from pretrained MODEL_ID\n",
    "        if (base / \"backbone_state.pth\").exists():\n",
    "            backbone = CLIPModel.from_pretrained(MODEL_ID).to(device)\n",
    "            state = torch.load(str(base / \"backbone_state.pth\"), map_location=\"cpu\")\n",
    "            try:\n",
    "                backbone.load_state_dict(state, strict=False)\n",
    "            except Exception:\n",
    "                # best-effort\n",
    "                pass\n",
    "            backbone.to(device)\n",
    "        else:\n",
    "            raise FileNotFoundError(\"No backbone checkpoint found in saved variant.\")\n",
    "\n",
    "    # rebuild head by running a dummy forward to get feat_dim\n",
    "    with torch.no_grad():\n",
    "        dummy_px = processor(images=Image.new(\"RGB\", (224, 224), \"white\"), return_tensors=\"pt\")[\"pixel_values\"].to(device)\n",
    "        feat = backbone.get_image_features(pixel_values=dummy_px)\n",
    "        feat_dim = feat.shape[-1]\n",
    "    # try to load saved head\n",
    "    head = nn.Linear(feat_dim, feat_dim)  # placeholder, will be replaced by correct size when loading\n",
    "    head_path = base / \"head_state.pth\"\n",
    "    if head_path.exists():\n",
    "        # load dict to infer out_features if possible\n",
    "        st = torch.load(str(head_path), map_location=\"cpu\")\n",
    "        # attempt to infer out_features from saved state keys (Linear: weight shape = [out_features, in_features])\n",
    "        if \"weight\" in st:\n",
    "            w = st[\"weight\"]\n",
    "            out_f, in_f = w.shape\n",
    "            head = nn.Linear(in_f, out_f).to(device)\n",
    "            head.load_state_dict(st)\n",
    "        else:\n",
    "            # fallback: try original behaviour — recreate typical classifier sized to known dataset if available\n",
    "            # create linear feat_dim -> num_classes if we can infer dataset from meta\n",
    "            meta_path = base / \"meta.json\"\n",
    "            num_classes = None\n",
    "            if meta_path.exists():\n",
    "                try:\n",
    "                    m = json.load(open(meta_path, \"r\", encoding=\"utf-8\"))\n",
    "                    ds = m.get(\"dataset\")\n",
    "                    if ds in DATASET_REGISTRY:\n",
    "                        num_classes = len(DATASET_REGISTRY[ds][\"class_names\"])\n",
    "                except Exception:\n",
    "                    pass\n",
    "            if num_classes is None:\n",
    "                raise RuntimeError(\"Cannot infer head shape; please recreate head manually before saving.\")\n",
    "            head = nn.Linear(feat_dim, num_classes).to(device)\n",
    "            head.load_state_dict(st)\n",
    "    else:\n",
    "        raise FileNotFoundError(\"No head_state.pth found for this variant.\")\n",
    "\n",
    "    head.to(device)\n",
    "    backbone.to(device)\n",
    "    backbone.eval(); head.eval()\n",
    "    return backbone, head\n",
    "\n",
    "def _extract_sample(sample):\n",
    "    if isinstance(sample, dict):\n",
    "        pixels = sample[\"pixel_values\"]\n",
    "        label = sample.get(\"label\")\n",
    "    else:\n",
    "        pixels, label = sample\n",
    "    px = torch.as_tensor(pixels).float()\n",
    "    if px.ndim == 4:\n",
    "        px = px.squeeze(0)\n",
    "    if px.ndim == 3 and px.shape[0] not in (1, 3) and px.shape[-1] == 3:\n",
    "        px = px.permute(2, 0, 1)\n",
    "    if px.ndim == 2:\n",
    "        px = px.unsqueeze(0)\n",
    "    if px.shape[0] == 1:\n",
    "        px = px.repeat(3, 1, 1)\n",
    "    if px.shape[1:] != (224, 224):\n",
    "        px = F.interpolate(\n",
    "            px.unsqueeze(0), size=(224, 224), mode=\"bilinear\", align_corners=False\n",
    "        ).squeeze(0)\n",
    "    if px.ndim != 3 or px.shape[0] != 3:\n",
    "        raise ValueError(f\"Unexpected pixel shape: {tuple(px.shape)}\")\n",
    "    return px, (int(label) if label is not None else None)\n",
    "\n",
    "\n",
    "def _prepare_batch(dataset_obj, indices: Sequence[int]):\n",
    "    tensors: List[torch.Tensor] = []\n",
    "    labels: List[Optional[int]] = []\n",
    "    actual_indices: List[int] = []\n",
    "    for idx in indices:\n",
    "        if idx < 0 or idx >= len(dataset_obj):\n",
    "            raise IndexError(f\"Index {idx} out of range for dataset length {len(dataset_obj)}\")\n",
    "        sample = dataset_obj[idx]\n",
    "        px, label = _extract_sample(sample)\n",
    "        tensors.append(px)\n",
    "        labels.append(label)\n",
    "        actual_indices.append(idx)\n",
    "    batch = torch.stack(tensors, dim=0)\n",
    "    return batch, labels, actual_indices\n",
    "\n",
    "def _resolve_dataset(dataset_name: str, split: str):\n",
    "    if dataset_name not in DATASET_REGISTRY:\n",
    "        raise ValueError(f\"Unknown dataset '{dataset_name}'.\")\n",
    "    info = DATASET_REGISTRY[dataset_name]\n",
    "    if split not in info[\"splits\"]:\n",
    "        raise ValueError(f\"Dataset '{dataset_name}' has no split '{split}'.\")\n",
    "    return info[\"splits\"][split], info[\"class_names\"], info[\"prompts\"]\n",
    "\n",
    "\n",
    "def free_gpu_safe(offload_registered: bool = True, skip_registered: bool = False):\n",
    "    \"\"\"\n",
    "    較保險的 GPU 清理：\n",
    "      - 若 offload_registered=True，會把 TRAINED_VARIANTS 中的 backbone/head/baseline 移到 CPU（保留 references）。\n",
    "      - 若 skip_registered=True，完全不 touch TRAINED_VARIANTS（適合你想避免清理的情況）。\n",
    "    \"\"\"\n",
    "    if not skip_registered:\n",
    "        try:\n",
    "            for v in TRAINED_VARIANTS.values():\n",
    "                for key in (\"backbone\", \"head\", \"baseline\"):\n",
    "                    obj = v.get(key)\n",
    "                    if obj is None:\n",
    "                        continue\n",
    "                    try:\n",
    "                        if offload_registered and hasattr(obj, \"to\"):\n",
    "                            obj.to(\"cpu\")\n",
    "                        if hasattr(obj, \"eval\"):\n",
    "                            obj.eval()\n",
    "                        for p in getattr(obj, \"parameters\", lambda: [])() or []:\n",
    "                            p.requires_grad = False\n",
    "                    except Exception:\n",
    "                        pass\n",
    "        except NameError:\n",
    "            pass\n",
    "\n",
    "    # 刪除常見臨時變數（但不刪除 TRAINED_VARIANTS 內的物件）\n",
    "    for name in (\"head\", \"optimizer\", \"model_lora\", \"vision_model_lora\", \"model_fb\"):\n",
    "        if name in globals():\n",
    "            try:\n",
    "                del globals()[name]\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d909324e",
   "metadata": {},
   "source": [
    "Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30a516d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def register_trained_variant(\n",
    "    name: str,\n",
    "    dataset: str,\n",
    "    backbone: \"CLIPModel\",\n",
    "    head: nn.Module,\n",
    "    baseline: Optional[\"CLIPModel\"] = None,\n",
    "    notes: Optional[str] = None,\n",
    "    save_to_disk: bool = True,\n",
    ") -> None:\n",
    "    \"\"\"紀錄訓練後的模型組態，供可視化比較使用。\"\"\"\n",
    "    baseline_model = baseline if baseline is not None else model\n",
    "    backbone.eval()\n",
    "    head.to(next(backbone.parameters()).device)\n",
    "    head.eval()\n",
    "    baseline_model.eval()\n",
    "    TRAINED_VARIANTS[name] = {\n",
    "        \"dataset\": dataset,\n",
    "        \"backbone\": backbone,\n",
    "        \"head\": head,\n",
    "        \"baseline\": baseline_model,\n",
    "        \"notes\": notes or \"\",\n",
    "    }\n",
    "    print(f\"Registered variant '{name}' (dataset={dataset}).\")\n",
    "    if save_to_disk:\n",
    "        try:\n",
    "            save_trained_variant_to_disk(name)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: failed saving variant '{name}' to disk: {e}\")\n",
    "\n",
    "\n",
    "def save_prediction_comparison_grid(\n",
    "    variant_name: str,\n",
    "    dataset: str,\n",
    "    *,\n",
    "    num_images: int,\n",
    "    indices: Sequence[int],\n",
    "    split: str = \"test\",\n",
    "    cols: int = 3,\n",
    "    top_k: int = 3,\n",
    "    font_size: int = 9,\n",
    "    output_path: Optional[str] = None,\n",
    "    show: bool = False,\n",
    "    device: Optional[str] = None,  # 新增：可指定載入/執行的 device (e.g. \"cuda\"/\"cpu\")\n",
    ") -> None:\n",
    "    \"\"\"產生原始 CLIP 與訓練後模型的比較圖，並儲存於指定路徑。\n",
    "\n",
    "    行為改動：\n",
    "    - 若 TRAINED_VARIANTS 中找不到 variant，會嘗試從 ./saved_variants 裡載入 (使用 load_trained_variant_from_disk)。\n",
    "    - 可用 device 參數強制把模型移到指定裝置。\n",
    "    \"\"\"\n",
    "    # 嘗試取得註冊的 variant；若不存在則嘗試從磁碟載入\n",
    "    variant = TRAINED_VARIANTS.get(variant_name)\n",
    "    if variant is None:\n",
    "        try:\n",
    "            tgt = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            backbone_loaded, head_loaded = load_trained_variant_from_disk(variant_name, device=tgt)\n",
    "            # 將載入的模型註冊到 TRAINED_VARIANTS，baseline 使用全域 model（zero-shot）\n",
    "            TRAINED_VARIANTS[variant_name] = {\n",
    "                \"dataset\": dataset,\n",
    "                \"backbone\": backbone_loaded,\n",
    "                \"head\": head_loaded,\n",
    "                \"baseline\": model,\n",
    "                \"notes\": \"\",\n",
    "            }\n",
    "            variant = TRAINED_VARIANTS[variant_name]\n",
    "            print(f\"Loaded variant '{variant_name}' from disk into TRAINED_VARIANTS (device={tgt}).\")\n",
    "        except Exception as e:\n",
    "            print(f\"Variant '{variant_name}' 尚未建立且無法從磁碟載入：{e}\")\n",
    "            return\n",
    "\n",
    "    dataset_obj, class_names, prompts = _resolve_dataset(dataset, split)\n",
    "    if len(dataset_obj) == 0:\n",
    "        print(f\"Dataset '{dataset}' ({split}) is empty.\")\n",
    "        return\n",
    "\n",
    "    if num_images <= 0:\n",
    "        raise ValueError(\"num_images 必須大於 0\")\n",
    "    if indices is None or len(indices) == 0:\n",
    "        raise ValueError(\"indices 至少需要一個項目\")\n",
    "\n",
    "    selected_indices = list(indices)[:num_images]\n",
    "    pixel_batch, labels, actual_indices = _prepare_batch(dataset_obj, selected_indices)\n",
    "\n",
    "    backbone = variant[\"backbone\"]\n",
    "    head = variant[\"head\"]\n",
    "    baseline_model = variant.get(\"baseline\", model)\n",
    "\n",
    "    # 如果使用者指定 device，強制移動模型至該裝置\n",
    "    if device is None:\n",
    "        # 預設 device 為 backbone 目前所在裝置（若可取得 parameter）\n",
    "        try:\n",
    "            default_dev = next(backbone.parameters()).device\n",
    "            device = str(default_dev)\n",
    "        except Exception:\n",
    "            device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    device = torch.device(device)\n",
    "    pixel_batch = pixel_batch.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        baseline_model = baseline_model.to(device)\n",
    "        backbone = backbone.to(device)\n",
    "        head = head.to(device)\n",
    "\n",
    "        baseline_model.eval()\n",
    "        backbone.eval()\n",
    "        head.eval()\n",
    "\n",
    "        text_inputs = processor(text=prompts, return_tensors=\"pt\", padding=True)\n",
    "        text_inputs = {k: v.to(device) for k, v in text_inputs.items()}\n",
    "        text_features = baseline_model.get_text_features(**text_inputs)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        image_features_orig = baseline_model.get_image_features(\n",
    "            pixel_values=pixel_batch.to(baseline_model.dtype)\n",
    "        )\n",
    "        image_features_orig = image_features_orig / image_features_orig.norm(dim=-1, keepdim=True)\n",
    "        logits_orig = image_features_orig @ text_features.T\n",
    "        probs_orig = logits_orig.softmax(dim=-1)\n",
    "\n",
    "        image_features_trained = backbone.get_image_features(\n",
    "            pixel_values=pixel_batch.to(backbone.dtype)\n",
    "        )\n",
    "        image_features_trained = image_features_trained / image_features_trained.norm(dim=-1, keepdim=True)\n",
    "        head_params = list(head.parameters())\n",
    "        head_dtype = head_params[0].dtype if head_params else image_features_trained.dtype\n",
    "        image_features_trained = image_features_trained.to(head_dtype)\n",
    "        logits_trained = head(image_features_trained)\n",
    "        probs_trained = logits_trained.softmax(dim=-1)\n",
    "\n",
    "    top_k = max(1, min(top_k, len(class_names)))\n",
    "    orig_vals, orig_idx = torch.topk(probs_orig, top_k, dim=1)\n",
    "    tr_vals, tr_idx = torch.topk(probs_trained, top_k, dim=1)\n",
    "\n",
    "    rows = math.ceil(num_images / cols)\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 5.2, rows * 3.2), constrained_layout=True)\n",
    "    # 調整子圖間距（改成較小的間距）。如要更緊密，調小 wspace/hspace（例如 0.15, 0.18）\n",
    "    fig.subplots_adjust(wspace=0.18, hspace=0.28)\n",
    "\n",
    "    if not isinstance(axes, np.ndarray):\n",
    "        axes = np.array([axes])\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    label_tensor = torch.tensor([(-1 if l is None else l) for l in labels])\n",
    "\n",
    "    for i, idx in enumerate(actual_indices):\n",
    "        ax = axes[i]\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "        img = pixel_batch[i].detach().float().cpu()\n",
    "        img = (img - img.min()) / max(1e-6, (img.max() - img.min()))\n",
    "        ax.imshow(img.permute(1, 2, 0))\n",
    "\n",
    "        gt = label_tensor[i].item()\n",
    "        if 0 <= gt < len(class_names):\n",
    "            title = f\"idx:{idx} GT: {class_names[gt]}\"\n",
    "        else:\n",
    "            title = f\"idx:{idx} GT: N/A\"\n",
    "        ax.set_title(title, fontsize=font_size + 1, pad=3)\n",
    "\n",
    "        # 改為上下排列：Zero-shot 在上方，Trained 在下方\n",
    "        text_x = 1.02\n",
    "        top_y = 0.92\n",
    "        dy = 0.085  # 每列文字垂直間距；可改小以擠更多行（但太小會重疊）\n",
    "        # 計算 trained header 的起始 y（在 zero-shot 之下）\n",
    "        trained_header_y = top_y - (top_k + 1) * dy - 0.06\n",
    "\n",
    "        ax.text(text_x, top_y, \"Zero-shot\", transform=ax.transAxes, fontsize=font_size, fontweight=\"bold\", ha=\"left\", va=\"top\")\n",
    "        # 列出 zero-shot 預測\n",
    "        for rank in range(top_k):\n",
    "            b_idx = int(orig_idx[i, rank])\n",
    "            b_score = float(orig_vals[i, rank]) * 100.0\n",
    "            base_color = \"#23a559\" if b_idx == gt and gt >= 0 else \"#d43f3a\"\n",
    "            ax.text(\n",
    "                text_x,\n",
    "                top_y - (rank + 1) * dy,\n",
    "                f\"{rank + 1}. {class_names[b_idx]} ({b_score:.1f}%)\",\n",
    "                transform=ax.transAxes,\n",
    "                fontsize=font_size,\n",
    "                color=base_color,\n",
    "                ha=\"left\",\n",
    "                va=\"top\",\n",
    "            )\n",
    "\n",
    "        # Trained header & 列表（放在 zero-shot 之下）\n",
    "        ax.text(text_x, trained_header_y, \"Trained\", transform=ax.transAxes, fontsize=font_size, fontweight=\"bold\", ha=\"left\", va=\"top\")\n",
    "        for rank in range(top_k):\n",
    "            t_idx = int(tr_idx[i, rank])\n",
    "            t_score = float(tr_vals[i, rank]) * 100.0\n",
    "            trained_color = \"#23a559\" if t_idx == gt and gt >= 0 else \"#d43f3a\"\n",
    "            ax.text(\n",
    "                text_x,\n",
    "                trained_header_y - (rank + 1) * dy,\n",
    "                f\"{rank + 1}. {class_names[t_idx]} ({t_score:.1f}%)\",\n",
    "                transform=ax.transAxes,\n",
    "                fontsize=font_size,\n",
    "                color=trained_color,\n",
    "                ha=\"left\",\n",
    "                va=\"top\",\n",
    "            )\n",
    "        \n",
    "\n",
    "    for j in range(len(actual_indices), len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "\n",
    "    if output_path is None:\n",
    "        output_path = f\"./data/{variant_name}_{dataset}_result.png\"\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    fig.savefig(output_path, dpi=300, bbox_inches=\"tight\")\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close(fig)\n",
    "    print(f\"Saved comparison grid to {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e8d754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Method: Linear Probing Flowers ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 [Train]: 100%|██████████| 1/1 [00:09<00:00,  9.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Loss: 4.6257 | Train Acc: 0.69% | Val Loss: 4.6172 | Val Acc: 1.86% | Time: 18.59s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 [Train]: 100%|██████████| 1/1 [00:09<00:00,  9.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Train Loss: 4.6169 | Train Acc: 2.35% | Val Loss: 4.6089 | Val Acc: 4.12% | Time: 18.06s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 [Train]: 100%|██████████| 1/1 [00:09<00:00,  9.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Train Loss: 4.6080 | Train Acc: 5.10% | Val Loss: 4.6007 | Val Acc: 8.14% | Time: 18.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 [Train]: 100%|██████████| 1/1 [00:09<00:00,  9.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Train Loss: 4.5991 | Train Acc: 9.12% | Val Loss: 4.5925 | Val Acc: 14.90% | Time: 18.05s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 [Train]: 100%|██████████| 1/1 [00:09<00:00,  9.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Train Loss: 4.5903 | Train Acc: 16.47% | Val Loss: 4.5843 | Val Acc: 22.94% | Time: 18.09s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 [Train]: 100%|██████████| 1/1 [00:09<00:00,  9.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Train Loss: 4.5814 | Train Acc: 26.67% | Val Loss: 4.5760 | Val Acc: 32.16% | Time: 18.06s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 [Train]: 100%|██████████| 1/1 [00:09<00:00,  9.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Train Loss: 4.5726 | Train Acc: 36.57% | Val Loss: 4.5678 | Val Acc: 41.86% | Time: 18.09s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 [Train]: 100%|██████████| 1/1 [00:09<00:00,  9.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Train Loss: 4.5637 | Train Acc: 47.55% | Val Loss: 4.5596 | Val Acc: 55.10% | Time: 18.10s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/30 [Train]: 100%|██████████| 1/1 [00:09<00:00,  9.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Train Loss: 4.5549 | Train Acc: 60.39% | Val Loss: 4.5514 | Val Acc: 65.00% | Time: 18.13s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 [Train]: 100%|██████████| 1/1 [00:09<00:00,  9.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Train Loss: 4.5461 | Train Acc: 72.84% | Val Loss: 4.5432 | Val Acc: 72.94% | Time: 18.13s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 [Train]: 100%|██████████| 1/1 [00:09<00:00,  9.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 - Train Loss: 4.5373 | Train Acc: 81.18% | Val Loss: 4.5350 | Val Acc: 79.90% | Time: 18.05s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 [Train]: 100%|██████████| 1/1 [00:09<00:00,  9.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 - Train Loss: 4.5285 | Train Acc: 86.86% | Val Loss: 4.5269 | Val Acc: 84.71% | Time: 18.21s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/30 [Train]: 100%|██████████| 1/1 [00:09<00:00,  9.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 - Train Loss: 4.5196 | Train Acc: 91.57% | Val Loss: 4.5187 | Val Acc: 87.94% | Time: 18.07s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/30 [Train]: 100%|██████████| 1/1 [00:09<00:00,  9.05s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"--- Starting Method: Linear Probing Flowers ---\")\n",
    "\n",
    "# We'll use the existing CLIP model (loaded earlier) and attach a linear classifier on top of image features.\n",
    "# Get image feature dimension with a dummy forward\n",
    "with torch.no_grad():\n",
    "    dummy_px = processor(images=Image.new(\"RGB\", (224, 224), \"white\"), return_tensors=\"pt\")[\"pixel_values\"].to(DEVICE)\n",
    "    feat = model.get_image_features(pixel_values=dummy_px)\n",
    "    feat_dim = feat.shape[-1]\n",
    "\n",
    "# Freeze full vision encoder to perform linear probing\n",
    "vision_model = model.vision_model\n",
    "visual_projection = getattr(model, 'visual_projection', None)\n",
    "for p in vision_model.parameters():\n",
    "    p.requires_grad = False\n",
    "if visual_projection is not None:\n",
    "    for p in visual_projection.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "# Simple linear head\n",
    "def make_head(num_classes):\n",
    "    return nn.Linear(feat_dim, num_classes).to(DEVICE)\n",
    "\n",
    "num_classes = len(flowers102_class_names)\n",
    "train_loader = flowers102_train_loader if 'flowers102_train_loader' in globals() else None\n",
    "val_loader = flowers102_val_loader if 'flowers102_val_loader' in globals() else None\n",
    "test_loader = flowers102_test_loader if 'flowers102_test_loader' in globals() else None\n",
    "class_names = flowers102_class_names\n",
    "\n",
    "head = make_head(num_classes)\n",
    "\n",
    "# training setup\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 1e-3\n",
    "optimizer = torch.optim.Adam(head.parameters(), lr=lr)\n",
    "train_losses, train_accuracies, val_losses, val_accuracies = [], [], [], []\n",
    "\n",
    "from time import time\n",
    "\n",
    "def evaluate(loader):\n",
    "    head.eval()\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            if isinstance(batch, dict):\n",
    "                imgs, labels = batch['pixel_values'], batch['label']\n",
    "            else:\n",
    "                imgs, labels = batch\n",
    "            if isinstance(imgs, list):\n",
    "                imgs = torch.stack(imgs, dim=0)\n",
    "            imgs = imgs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            feats = model.get_image_features(pixel_values=imgs)\n",
    "            feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "            logits = head(feats)\n",
    "            loss = criterion(logits, labels)\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            loss_sum += loss.item() * labels.size(0)\n",
    "    return loss_sum / total, correct / total\n",
    "\n",
    "# If no training loader is available, skip training and just run evaluation on test set\n",
    "if train_loader is None:\n",
    "    print('No training loader prepared for the chosen dataset; running evaluation on test set only')\n",
    "    test_loss, test_acc = evaluate(test_loader)\n",
    "    print(f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc*100:.2f}%\")\n",
    "else:\n",
    "    # training loop\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        head.train()\n",
    "        epoch_start = time()\n",
    "        running_loss = 0.0\n",
    "        running_correct = 0\n",
    "        seen = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Train]\"):\n",
    "            if isinstance(batch, dict):\n",
    "                imgs, labels = batch['pixel_values'], batch['label']\n",
    "            else:\n",
    "                imgs, labels = batch\n",
    "            if isinstance(imgs, list):\n",
    "                imgs = torch.stack(imgs, dim=0)\n",
    "            imgs = imgs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            feats = model.get_image_features(pixel_values=imgs)\n",
    "            feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "            logits = head(feats)\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "            running_correct += (preds == labels).sum().item()\n",
    "            seen += labels.size(0)\n",
    "\n",
    "        train_loss = running_loss / max(1, seen)\n",
    "        train_acc = running_correct / max(1, seen)\n",
    "        if val_loader is not None:\n",
    "            val_loss, val_acc = evaluate(val_loader)\n",
    "        else:\n",
    "            val_loss, val_acc = evaluate(test_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "        epoch_end = time()\n",
    "        print(f\"Epoch {epoch+1} - Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}% | Val Loss: {val_loss:.4f} | Val Acc: {val_acc*100:.2f}% | Time: {epoch_end-epoch_start:.2f}s\")\n",
    "\n",
    "    save_training_artifacts(\"linear_flowers\", \"Linear Probing Flowers\", train_losses, train_accuracies, val_losses, val_accuracies)\n",
    "\n",
    "    # final test\n",
    "    test_loss, test_acc = evaluate(test_loader)\n",
    "    print(f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc*100:.2f}%\")\n",
    "\n",
    "register_trained_variant(\n",
    "    name=\"linear_flowers\",\n",
    "    dataset=\"flowers102\",\n",
    "    backbone=model,\n",
    "    head=head,\n",
    "    baseline=model,\n",
    "    notes=\"Linear probing on Flowers102\"\n",
    ")\n",
    "print('Linear probing Flowers finished')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8391e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1220719/3945290448.py:138: UserWarning: This figure was using a layout engine that is incompatible with subplots_adjust and/or tight_layout; not calling subplots_adjust.\n",
      "  fig.subplots_adjust(wspace=0.18, hspace=0.28)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved comparison grid to ./data/linear_flowers_result.png\n"
     ]
    }
   ],
   "source": [
    "# 線性探針模型（Flowers102）的原始與訓練後預測比較\n",
    "save_prediction_comparison_grid(\n",
    "    \"linear_flowers\",\n",
    "    dataset=\"flowers102\",\n",
    "    num_images=16,\n",
    "    indices=[12, 23, 85, 91, 150, 151, 274, 286, 545, 1249, 1666, 1997, 2268, 2290, 3916, 3917],\n",
    "    split=\"test\",\n",
    "    cols=4,\n",
    "    top_k=5,\n",
    "    output_path=\"./data/linear_flowers_result.png\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbc075f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Method: Linear Probing Birds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 [Train]:   0%|          | 0/1349 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 [Train]:  31%|███▏      | 423/1349 [00:22<00:48, 19.18it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 97\u001b[0m\n\u001b[1;32m     94\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     96\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 97\u001b[0m     running_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     seen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    100\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, seen)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"--- Starting Method: Linear Probing Birds ---\")\n",
    "\n",
    "# We'll use the existing CLIP model (loaded earlier) and attach a linear classifier on top of image features.\n",
    "# Get image feature dimension with a dummy forward\n",
    "with torch.no_grad():\n",
    "    dummy_px = processor(images=Image.new(\"RGB\", (224, 224), \"white\"), return_tensors=\"pt\")[\"pixel_values\"].to(DEVICE)\n",
    "    feat = model.get_image_features(pixel_values=dummy_px)\n",
    "    feat_dim = feat.shape[-1]\n",
    "\n",
    "# Freeze full vision encoder to perform linear probing\n",
    "vision_model = model.vision_model\n",
    "visual_projection = getattr(model, 'visual_projection', None)\n",
    "for p in vision_model.parameters():\n",
    "    p.requires_grad = False\n",
    "if visual_projection is not None:\n",
    "    for p in visual_projection.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "# Simple linear head\n",
    "def make_head(num_classes):\n",
    "    return nn.Linear(feat_dim, num_classes).to(DEVICE)\n",
    "\n",
    "num_classes = len(cub_bird_class_names)\n",
    "# If train/val splits aren't prepared, fall back to test-only evaluation\n",
    "train_loader = cub_bird_train_loader if 'cub_bird_train_loader' in globals() else None\n",
    "val_loader = cub_bird_val_loader if 'cub_bird_val_loader' in globals() else None\n",
    "test_loader = cub_bird_test_loader if 'cub_bird_test_loader' in globals() else None\n",
    "class_names = cub_bird_class_names\n",
    "\n",
    "head = make_head(num_classes)\n",
    "\n",
    "# training setup\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 1e-3\n",
    "optimizer = torch.optim.Adam(head.parameters(), lr=lr)\n",
    "train_losses, train_accuracies, val_losses, val_accuracies = [], [], [], []\n",
    "\n",
    "from time import time\n",
    "\n",
    "def evaluate(loader):\n",
    "    head.eval()\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            if isinstance(batch, dict):\n",
    "                imgs, labels = batch['pixel_values'], batch['label']\n",
    "            else:\n",
    "                imgs, labels = batch\n",
    "            if isinstance(imgs, list):\n",
    "                imgs = torch.stack(imgs, dim=0)\n",
    "            imgs = imgs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            feats = model.get_image_features(pixel_values=imgs)\n",
    "            feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "            logits = head(feats)\n",
    "            loss = criterion(logits, labels)\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            loss_sum += loss.item() * labels.size(0)\n",
    "    return loss_sum / total, correct / total\n",
    "\n",
    "# If no training loader is available, skip training and just run evaluation on test set\n",
    "if train_loader is None:\n",
    "    print('No training loader prepared for the chosen dataset; running evaluation on test set only')\n",
    "    test_loss, test_acc = evaluate(test_loader)\n",
    "    print(f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc*100:.2f}%\")\n",
    "else:\n",
    "    # training loop\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        head.train()\n",
    "        epoch_start = time()\n",
    "        running_loss = 0.0\n",
    "        running_correct = 0\n",
    "        seen = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Train]\"):\n",
    "            if isinstance(batch, dict):\n",
    "                imgs, labels = batch['pixel_values'], batch['label']\n",
    "            else:\n",
    "                imgs, labels = batch\n",
    "            if isinstance(imgs, list):\n",
    "                imgs = torch.stack(imgs, dim=0)\n",
    "            imgs = imgs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            feats = model.get_image_features(pixel_values=imgs)\n",
    "            feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "            logits = head(feats)\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "            running_correct += (preds == labels).sum().item()\n",
    "            seen += labels.size(0)\n",
    "\n",
    "        train_loss = running_loss / max(1, seen)\n",
    "        train_acc = running_correct / max(1, seen)\n",
    "        if val_loader is not None:\n",
    "            val_loss, val_acc = evaluate(val_loader)\n",
    "        else:\n",
    "            val_loss, val_acc = evaluate(test_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "        epoch_end = time()\n",
    "        print(f\"Epoch {epoch+1} - Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}% | Val Loss: {val_loss:.4f} | Val Acc: {val_acc*100:.2f}% | Time: {epoch_end-epoch_start:.2f}s\")\n",
    "\n",
    "    save_training_artifacts(\"linear_birds\", \"Linear Probing Birds\", train_losses, train_accuracies, val_losses, val_accuracies)\n",
    "\n",
    "    # final test\n",
    "    test_loss, test_acc = evaluate(test_loader)\n",
    "    print(f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc*100:.2f}%\")\n",
    "\n",
    "register_trained_variant(\n",
    "    name=\"linear_birds\",\n",
    "    dataset=\"cub_bird\",\n",
    "    backbone=model,\n",
    "    head=head,\n",
    "    baseline=model,\n",
    "    notes=\"Linear probing on CUB-200-2011\"\n",
    ")\n",
    "print('Linear probing Birds finished')\n",
    "\n",
    "free_gpu_safe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20606de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1220719/3945290448.py:138: UserWarning: This figure was using a layout engine that is incompatible with subplots_adjust and/or tight_layout; not calling subplots_adjust.\n",
      "  fig.subplots_adjust(wspace=0.18, hspace=0.28)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved comparison grid to ./data/linear_birds_result.png\n"
     ]
    }
   ],
   "source": [
    "# 線性探針模型（CUB-200-2011）的原始與訓練後預測比較\n",
    "save_prediction_comparison_grid(\n",
    "    \"linear_birds\",\n",
    "    dataset=\"cub_bird\",\n",
    "    num_images=16,\n",
    "    indices=[25, 45, 123, 125, 128, 129, 140, 143, 887, 888, 1384, 1385, 2985, 4010, 5569, 5570],\n",
    "    split=\"test\",\n",
    "    cols=4,\n",
    "    top_k=5,\n",
    "    output_path=\"./data/linear_birds_result.png\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c441f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "flowers102_train_loader = DataLoader(\n",
    "    flowers102_train_dts,\n",
    "    batch_size=FINE_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "flowers102_val_loader = DataLoader(\n",
    "    flowers102_val_dts,\n",
    "    batch_size=FINE_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "flowers102_test_loader = DataLoader(\n",
    "    flowers102_test_dts,\n",
    "    batch_size=FINE_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "cub_bird_train_loader = DataLoader(\n",
    "    cub_bird_train_dts,\n",
    "    batch_size=FINE_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    collate_fn=hf_collate_fn\n",
    ")\n",
    "\n",
    "cub_bird_val_loader = DataLoader(\n",
    "    cub_bird_val_dts,\n",
    "    batch_size=FINE_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    collate_fn=hf_collate_fn\n",
    ")\n",
    "\n",
    "cub_bird_test_loader = DataLoader(\n",
    "    cub_bird_test_dts,\n",
    "    batch_size=FINE_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    collate_fn=hf_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9f7832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Method: LoRA Fine-Tuning Flowers ---\n",
      "LoRA Model - Trainable Parameters:\n",
      "trainable params: 294,912 || all params: 303,474,688 || trainable%: 0.0972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LoRA Epoch 1/20 [Train]: 100%|██████████| 128/128 [00:27<00:00,  4.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA Epoch 1 - Train Loss: 4.5181 | Train Acc: 34.12% | Val Loss: 4.3309 | Val Acc: 79.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LoRA Epoch 2/20 [Train]:  20%|██        | 26/128 [00:05<00:23,  4.37it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 105\u001b[0m\n\u001b[1;32m    103\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    104\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 105\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    106\u001b[0m running_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (preds \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    107\u001b[0m seen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"--- Starting Method: LoRA Fine-Tuning Flowers ---\")\n",
    "\n",
    "variant_backbone = None\n",
    "variant_notes = \"LoRA fine-tuning on Flowers102\"\n",
    "\n",
    "if 'model' not in globals():\n",
    "    model = CLIPModel.from_pretrained(MODEL_ID).to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "train_loader = flowers102_train_loader if 'flowers102_train_loader' in globals() else None\n",
    "val_loader = flowers102_val_loader if 'flowers102_val_loader' in globals() else None\n",
    "test_loader = flowers102_test_loader if 'flowers102_test_loader' in globals() else None\n",
    "num_classes = len(flowers102_class_names)\n",
    "\n",
    "# Try to apply LoRA to the vision encoder. If PEFT is unavailable or fails, fall back to tuning the visual projection + head.\n",
    "try:\n",
    "    # load a fresh CLIP model to avoid interference\n",
    "    model_lora = CLIPModel.from_pretrained(MODEL_ID).to(DEVICE)\n",
    "    variant_backbone = model_lora\n",
    "    vision_model = model_lora.vision_model\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=2,\n",
    "        lora_alpha=4,\n",
    "        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "    )\n",
    "\n",
    "    vision_model_lora = get_peft_model(vision_model, lora_config)\n",
    "    model_lora.vision_model = vision_model_lora\n",
    "    print(\"LoRA Model - Trainable Parameters:\")\n",
    "    vision_model_lora.print_trainable_parameters()\n",
    "\n",
    "    # freeze visual projection and text encoder if present\n",
    "    if hasattr(model_lora, 'visual_projection') and model_lora.visual_projection is not None:\n",
    "        for p in model_lora.visual_projection.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    # prepare head\n",
    "    with torch.no_grad():\n",
    "        dummy_px = processor(images=Image.new(\"RGB\", (224, 224), \"white\"), return_tensors=\"pt\")[\"pixel_values\"].to(DEVICE)\n",
    "        feat = model_lora.get_image_features(pixel_values=dummy_px)\n",
    "        feat_dim = feat.shape[-1]\n",
    "    head = nn.Linear(feat_dim, num_classes).to(DEVICE)\n",
    "\n",
    "    params = list(filter(lambda p: p.requires_grad, model_lora.parameters())) + list(head.parameters())\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(params, lr=5e-4)\n",
    "\n",
    "    def eval_model(m):\n",
    "        m.eval(); head.eval()\n",
    "        total, correct, loss_sum = 0, 0, 0.0\n",
    "        loader = val_loader if val_loader is not None else test_loader\n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                if isinstance(batch, dict):\n",
    "                    imgs, labels = batch['pixel_values'], batch['label']\n",
    "                else:\n",
    "                    imgs, labels = batch\n",
    "                if isinstance(imgs, list):\n",
    "                    imgs = torch.stack(imgs, dim=0)\n",
    "                imgs = imgs.to(DEVICE)\n",
    "                labels = labels.to(DEVICE)\n",
    "                feats = m.get_image_features(pixel_values=imgs)\n",
    "                feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "                logits = head(feats)\n",
    "                loss = criterion(logits, labels)\n",
    "                preds = logits.argmax(dim=-1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "                loss_sum += loss.item() * labels.size(0)\n",
    "        return loss_sum/total, correct/total\n",
    "\n",
    "    # train LoRA + head (if train_loader exists)\n",
    "    if train_loader is None:\n",
    "        print('No training loader available for LoRA path; running quick eval instead')\n",
    "        tloss, tacc = eval_model(model_lora)\n",
    "        print(f\"Val/Test Loss: {tloss:.4f} | Acc: {tacc*100:.2f}%\")\n",
    "    else:\n",
    "        train_losses, train_accuracies, val_losses, val_accuracies = [], [], [], []\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            model_lora.train(); head.train()\n",
    "            running_loss = 0.0\n",
    "            running_correct = 0\n",
    "            seen = 0\n",
    "            for batch in tqdm(train_loader, desc=f\"LoRA Epoch {epoch+1}/{NUM_EPOCHS} [Train]\"):\n",
    "                if isinstance(batch, dict):\n",
    "                    imgs, labels = batch['pixel_values'], batch['label']\n",
    "                else:\n",
    "                    imgs, labels = batch\n",
    "                if isinstance(imgs, list):\n",
    "                    imgs = torch.stack(imgs, dim=0)\n",
    "                imgs = imgs.to(DEVICE)\n",
    "                labels = labels.to(DEVICE)\n",
    "                feats = model_lora.get_image_features(pixel_values=imgs)\n",
    "                feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "                logits = head(feats)\n",
    "                preds = logits.argmax(dim=-1)\n",
    "                loss = criterion(logits, labels)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * labels.size(0)\n",
    "                running_correct += (preds == labels).sum().item()\n",
    "                seen += labels.size(0)\n",
    "            train_loss = running_loss / max(1, seen)\n",
    "            train_acc = running_correct / max(1, seen)\n",
    "            val_loss, val_acc = eval_model(model_lora)\n",
    "            train_losses.append(train_loss)\n",
    "            train_accuracies.append(train_acc)\n",
    "            val_losses.append(val_loss)\n",
    "            val_accuracies.append(val_acc)\n",
    "            print(f\"LoRA Epoch {epoch+1} - Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}% | Val Loss: {val_loss:.4f} | Val Acc: {val_acc*100:.2f}%\")\n",
    "        save_training_artifacts(\"finetuning_flowers\", \"LoRA Fine-Tuning Flowers\", train_losses, train_accuracies, val_losses, val_accuracies)\n",
    "        test_loss, test_acc = eval_model(model_lora)\n",
    "        print(f\"LoRA Test Loss: {test_loss:.4f} | Test Acc: {test_acc*100:.2f}%\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"LoRA path failed or PEFT not available; falling back to tuning visual projection + head. Error:\", e)\n",
    "    # fallback approach\n",
    "    model_fb = CLIPModel.from_pretrained(MODEL_ID).to(DEVICE)\n",
    "    variant_backbone = model_fb\n",
    "    variant_notes = \"Fallback fine-tuning on Flowers102\"\n",
    "    if hasattr(model_fb, 'visual_projection') and model_fb.visual_projection is not None:\n",
    "        for p in model_fb.visual_projection.parameters():\n",
    "            p.requires_grad = True\n",
    "    for p in model_fb.vision_model.parameters():\n",
    "        p.requires_grad = False\n",
    "    with torch.no_grad():\n",
    "        dummy_px = processor(images=Image.new(\"RGB\", (224, 224), \"white\"), return_tensors=\"pt\")[\"pixel_values\"].to(DEVICE)\n",
    "        feat = model_fb.get_image_features(pixel_values=dummy_px)\n",
    "        feat_dim = feat.shape[-1]\n",
    "    head = nn.Linear(feat_dim, num_classes).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(list(head.parameters()) + list(filter(lambda p: p.requires_grad, model_fb.parameters())), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if train_loader is None:\n",
    "        print('No training loader available for fallback; exiting fallback')\n",
    "    else:\n",
    "        fallback_train_losses, fallback_train_accuracies = [], []\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            model_fb.train(); head.train()\n",
    "            running_loss = 0.0\n",
    "            running_correct = 0\n",
    "            seen = 0\n",
    "            for batch in tqdm(train_loader, desc=f\"Fallback Epoch {epoch+1}/{NUM_EPOCHS} [Train]\"):\n",
    "                if isinstance(batch, dict):\n",
    "                    imgs, labels = batch['pixel_values'], batch['label']\n",
    "                else:\n",
    "                    imgs, labels = batch\n",
    "                if isinstance(imgs, list):\n",
    "                    imgs = torch.stack(imgs, dim=0)\n",
    "                imgs = imgs.to(DEVICE); labels = labels.to(DEVICE)\n",
    "                feats = model_fb.get_image_features(pixel_values=imgs)\n",
    "                feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "                logits = head(feats)\n",
    "                preds = logits.argmax(dim=-1)\n",
    "                loss = criterion(logits, labels)\n",
    "                optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "                running_loss += loss.item()*labels.size(0)\n",
    "                running_correct += (preds == labels).sum().item()\n",
    "                seen += labels.size(0)\n",
    "            train_loss = running_loss / max(1, seen)\n",
    "            train_acc = running_correct / max(1, seen)\n",
    "            fallback_train_losses.append(train_loss)\n",
    "            fallback_train_accuracies.append(train_acc)\n",
    "            print(f\"Fallback epoch {epoch+1} - Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "        save_training_artifacts(\"finetuning_flowers\", \"Fallback Fine-Tuning Flowers\", fallback_train_losses, fallback_train_accuracies)\n",
    "        # Evaluate fallback\n",
    "        def eval_fb():\n",
    "            head.eval(); model_fb.eval()\n",
    "            total, correct = 0, 0\n",
    "            loader = val_loader if val_loader is not None else test_loader\n",
    "            with torch.no_grad():\n",
    "                for batch in loader:\n",
    "                    if isinstance(batch, dict):\n",
    "                        imgs, labels = batch['pixel_values'], batch['label']\n",
    "                    else:\n",
    "                        imgs, labels = batch\n",
    "                    if isinstance(imgs, list):\n",
    "                        imgs = torch.stack(imgs, dim=0)\n",
    "                    imgs = imgs.to(DEVICE); labels = labels.to(DEVICE)\n",
    "                    feats = model_fb.get_image_features(pixel_values=imgs)\n",
    "                    feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "                    preds = head(feats).argmax(dim=-1)\n",
    "                    correct += (preds == labels).sum().item(); total += labels.size(0)\n",
    "            return correct/total\n",
    "        print(f\"Fallback Val Acc: {eval_fb()*100:.2f}%\")\n",
    "\n",
    "if variant_backbone is not None:\n",
    "    register_trained_variant(\n",
    "        name=\"lora_flowers\",\n",
    "        dataset=\"flowers102\",\n",
    "        backbone=variant_backbone,\n",
    "        head=head,\n",
    "        baseline=model,\n",
    "        notes=variant_notes\n",
    "    )\n",
    "print('LoRA Flowers finished')\n",
    "\n",
    "\n",
    "free_gpu_safe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e7938e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1220719/3945290448.py:138: UserWarning: This figure was using a layout engine that is incompatible with subplots_adjust and/or tight_layout; not calling subplots_adjust.\n",
      "  fig.subplots_adjust(wspace=0.18, hspace=0.28)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved comparison grid to ./data/fine_flowers_result.png\n"
     ]
    }
   ],
   "source": [
    "# LoRA 微調後（Flowers102）的預測比較\n",
    "save_prediction_comparison_grid(\n",
    "    \"lora_flowers\",\n",
    "    dataset=\"flowers102\",\n",
    "    num_images=16,\n",
    "    indices=[12, 23, 85, 91, 150, 151, 274, 286, 545, 1249, 1666, 1997, 2268, 2290, 3916, 3917],\n",
    "    split=\"test\",\n",
    "    cols=4,\n",
    "    top_k=5,\n",
    "    output_path=\"./data/fine_flowers_result.png\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718945c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Method: LoRA Fine-Tuning Birds ---\n",
      "LoRA Model - Trainable Parameters:\n",
      "trainable params: 294,912 || all params: 303,474,688 || trainable%: 0.0972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LoRA Epoch 1/20 [Train]:   3%|▎         | 41/1349 [00:05<02:35,  8.43it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LoRA Epoch 1/20 [Train]: 100%|██████████| 1349/1349 [02:40<00:00,  8.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA Epoch 1 - Train Loss: 4.6310 | Train Acc: 31.57% | Val Loss: 3.9801 | Val Acc: 47.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LoRA Epoch 2/20 [Train]: 100%|██████████| 1349/1349 [02:40<00:00,  8.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA Epoch 2 - Train Loss: 3.3752 | Train Acc: 58.42% | Val Loss: 2.9429 | Val Acc: 59.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LoRA Epoch 3/20 [Train]: 100%|██████████| 1349/1349 [02:40<00:00,  8.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA Epoch 3 - Train Loss: 2.3922 | Train Acc: 72.43% | Val Loss: 2.1485 | Val Acc: 68.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LoRA Epoch 4/20 [Train]: 100%|██████████| 1349/1349 [02:40<00:00,  8.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA Epoch 4 - Train Loss: 1.6711 | Train Acc: 82.68% | Val Loss: 1.6258 | Val Acc: 75.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LoRA Epoch 5/20 [Train]: 100%|██████████| 1349/1349 [02:40<00:00,  8.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA Epoch 5 - Train Loss: 1.1580 | Train Acc: 87.84% | Val Loss: 1.2997 | Val Acc: 78.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LoRA Epoch 6/20 [Train]: 100%|██████████| 1349/1349 [02:40<00:00,  8.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA Epoch 6 - Train Loss: 0.8037 | Train Acc: 91.84% | Val Loss: 1.1025 | Val Acc: 76.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LoRA Epoch 7/20 [Train]: 100%|██████████| 1349/1349 [02:40<00:00,  8.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA Epoch 7 - Train Loss: 0.5635 | Train Acc: 93.75% | Val Loss: 0.9024 | Val Acc: 82.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LoRA Epoch 8/20 [Train]: 100%|██████████| 1349/1349 [02:40<00:00,  8.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA Epoch 8 - Train Loss: 0.3978 | Train Acc: 96.00% | Val Loss: 0.8354 | Val Acc: 80.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LoRA Epoch 9/20 [Train]: 100%|██████████| 1349/1349 [02:40<00:00,  8.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA Epoch 9 - Train Loss: 0.2919 | Train Acc: 96.63% | Val Loss: 0.7617 | Val Acc: 80.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LoRA Epoch 10/20 [Train]: 100%|██████████| 1349/1349 [02:39<00:00,  8.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA Epoch 10 - Train Loss: 0.2154 | Train Acc: 97.48% | Val Loss: 0.7317 | Val Acc: 81.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LoRA Epoch 11/20 [Train]: 100%|██████████| 1349/1349 [02:40<00:00,  8.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA Epoch 11 - Train Loss: 0.1720 | Train Acc: 97.85% | Val Loss: 0.8406 | Val Acc: 79.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LoRA Epoch 12/20 [Train]: 100%|██████████| 1349/1349 [02:40<00:00,  8.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA Epoch 12 - Train Loss: 0.1422 | Train Acc: 98.13% | Val Loss: 0.7594 | Val Acc: 78.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LoRA Epoch 13/20 [Train]: 100%|██████████| 1349/1349 [02:40<00:00,  8.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA Epoch 13 - Train Loss: 0.1561 | Train Acc: 97.40% | Val Loss: 0.7076 | Val Acc: 80.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LoRA Epoch 14/20 [Train]: 100%|██████████| 1349/1349 [02:40<00:00,  8.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA Epoch 14 - Train Loss: 0.1084 | Train Acc: 98.46% | Val Loss: 0.8076 | Val Acc: 77.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LoRA Epoch 15/20 [Train]: 100%|██████████| 1349/1349 [02:40<00:00,  8.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA Epoch 15 - Train Loss: 0.1175 | Train Acc: 98.07% | Val Loss: 0.7356 | Val Acc: 79.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LoRA Epoch 16/20 [Train]: 100%|██████████| 1349/1349 [02:39<00:00,  8.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA Epoch 16 - Train Loss: 0.1071 | Train Acc: 98.02% | Val Loss: 0.8299 | Val Acc: 78.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LoRA Epoch 17/20 [Train]: 100%|██████████| 1349/1349 [02:40<00:00,  8.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA Epoch 17 - Train Loss: 0.1012 | Train Acc: 98.22% | Val Loss: 0.7124 | Val Acc: 80.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LoRA Epoch 18/20 [Train]: 100%|██████████| 1349/1349 [02:40<00:00,  8.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA Epoch 18 - Train Loss: 0.0974 | Train Acc: 98.26% | Val Loss: 0.7532 | Val Acc: 79.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LoRA Epoch 19/20 [Train]: 100%|██████████| 1349/1349 [02:40<00:00,  8.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA Epoch 19 - Train Loss: 0.0660 | Train Acc: 98.96% | Val Loss: 0.7571 | Val Acc: 81.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LoRA Epoch 20/20 [Train]: 100%|██████████| 1349/1349 [02:40<00:00,  8.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA Epoch 20 - Train Loss: 0.1043 | Train Acc: 97.94% | Val Loss: 0.7523 | Val Acc: 80.33%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA Test Loss: 0.7523 | Test Acc: 80.33%\n",
      "Registered variant 'lora_birds' (dataset=cub_bird).\n",
      "Saved variant 'lora_birds' to saved_variants/lora_birds\n",
      "LoRA Birds finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"--- Starting Method: LoRA Fine-Tuning Birds ---\")\n",
    "\n",
    "variant_backbone = None\n",
    "variant_notes = \"LoRA fine-tuning on CUB-200-2011\"\n",
    "\n",
    "if 'model' not in globals():\n",
    "    model = CLIPModel.from_pretrained(MODEL_ID).to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "train_loader = cub_bird_train_loader if 'cub_bird_train_loader' in globals() else None\n",
    "val_loader = cub_bird_val_loader if 'cub_bird_val_loader' in globals() else None\n",
    "test_loader = cub_bird_test_loader if 'cub_bird_test_loader' in globals() else None\n",
    "num_classes = len(cub_bird_class_names)\n",
    "\n",
    "# Try to apply LoRA to the vision encoder. If PEFT is unavailable or fails, fall back to tuning the visual projection + head.\n",
    "try:\n",
    "    # load a fresh CLIP model to avoid interference\n",
    "    model_lora = CLIPModel.from_pretrained(MODEL_ID).to(DEVICE)\n",
    "    variant_backbone = model_lora\n",
    "    vision_model = model_lora.vision_model\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=2,\n",
    "        lora_alpha=4,\n",
    "        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "    )\n",
    "\n",
    "    vision_model_lora = get_peft_model(vision_model, lora_config)\n",
    "    model_lora.vision_model = vision_model_lora\n",
    "    print(\"LoRA Model - Trainable Parameters:\")\n",
    "    vision_model_lora.print_trainable_parameters()\n",
    "\n",
    "    # freeze visual projection and text encoder if present\n",
    "    if hasattr(model_lora, 'visual_projection') and model_lora.visual_projection is not None:\n",
    "        for p in model_lora.visual_projection.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    # prepare head\n",
    "    with torch.no_grad():\n",
    "        dummy_px = processor(images=Image.new(\"RGB\", (224, 224), \"white\"), return_tensors=\"pt\")[\"pixel_values\"].to(DEVICE)\n",
    "        feat = model_lora.get_image_features(pixel_values=dummy_px)\n",
    "        feat_dim = feat.shape[-1]\n",
    "    head = nn.Linear(feat_dim, num_classes).to(DEVICE)\n",
    "\n",
    "    params = list(filter(lambda p: p.requires_grad, model_lora.parameters())) + list(head.parameters())\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(params, lr=5e-4)\n",
    "\n",
    "    def eval_model(m):\n",
    "        m.eval(); head.eval()\n",
    "        total, correct, loss_sum = 0, 0, 0.0\n",
    "        loader = val_loader if val_loader is not None else test_loader\n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                if isinstance(batch, dict):\n",
    "                    imgs, labels = batch['pixel_values'], batch['label']\n",
    "                else:\n",
    "                    imgs, labels = batch\n",
    "                if isinstance(imgs, list):\n",
    "                    imgs = torch.stack(imgs, dim=0)\n",
    "                imgs = imgs.to(DEVICE)\n",
    "                labels = labels.to(DEVICE)\n",
    "                feats = m.get_image_features(pixel_values=imgs)\n",
    "                feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "                logits = head(feats)\n",
    "                loss = criterion(logits, labels)\n",
    "                preds = logits.argmax(dim=-1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "                loss_sum += loss.item() * labels.size(0)\n",
    "        return loss_sum/total, correct/total\n",
    "\n",
    "    # train LoRA + head (if train_loader exists)\n",
    "    if train_loader is None:\n",
    "        print('No training loader available for LoRA path; running quick eval instead')\n",
    "        tloss, tacc = eval_model(model_lora)\n",
    "        print(f\"Val/Test Loss: {tloss:.4f} | Acc: {tacc*100:.2f}%\")\n",
    "    else:\n",
    "        train_losses, train_accuracies, val_losses, val_accuracies = [], [], [], []\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            model_lora.train(); head.train()\n",
    "            running_loss = 0.0\n",
    "            running_correct = 0\n",
    "            seen = 0\n",
    "            for batch in tqdm(train_loader, desc=f\"LoRA Epoch {epoch+1}/{NUM_EPOCHS} [Train]\"):\n",
    "                if isinstance(batch, dict):\n",
    "                    imgs, labels = batch['pixel_values'], batch['label']\n",
    "                else:\n",
    "                    imgs, labels = batch\n",
    "                if isinstance(imgs, list):\n",
    "                    imgs = torch.stack(imgs, dim=0)\n",
    "                imgs = imgs.to(DEVICE)\n",
    "                labels = labels.to(DEVICE)\n",
    "                feats = model_lora.get_image_features(pixel_values=imgs)\n",
    "                feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "                logits = head(feats)\n",
    "                preds = logits.argmax(dim=-1)\n",
    "                loss = criterion(logits, labels)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * labels.size(0)\n",
    "                running_correct += (preds == labels).sum().item()\n",
    "                seen += labels.size(0)\n",
    "            train_loss = running_loss / max(1, seen)\n",
    "            train_acc = running_correct / max(1, seen)\n",
    "            val_loss, val_acc = eval_model(model_lora)\n",
    "            train_losses.append(train_loss)\n",
    "            train_accuracies.append(train_acc)\n",
    "            val_losses.append(val_loss)\n",
    "            val_accuracies.append(val_acc)\n",
    "            print(f\"LoRA Epoch {epoch+1} - Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}% | Val Loss: {val_loss:.4f} | Val Acc: {val_acc*100:.2f}%\")\n",
    "        save_training_artifacts(\"finetuning_birds\", \"LoRA Fine-Tuning Birds\", train_losses, train_accuracies, val_losses, val_accuracies)\n",
    "        test_loss, test_acc = eval_model(model_lora)\n",
    "        print(f\"LoRA Test Loss: {test_loss:.4f} | Test Acc: {test_acc*100:.2f}%\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"LoRA path failed or PEFT not available; falling back to tuning visual projection + head. Error:\", e)\n",
    "    # fallback approach\n",
    "    model_fb = CLIPModel.from_pretrained(MODEL_ID).to(DEVICE)\n",
    "    variant_backbone = model_fb\n",
    "    variant_notes = \"Fallback fine-tuning on CUB-200-2011\"\n",
    "    if hasattr(model_fb, 'visual_projection') and model_fb.visual_projection is not None:\n",
    "        for p in model_fb.visual_projection.parameters():\n",
    "            p.requires_grad = True\n",
    "    for p in model_fb.vision_model.parameters():\n",
    "        p.requires_grad = False\n",
    "    with torch.no_grad():\n",
    "        dummy_px = processor(images=Image.new(\"RGB\", (224, 224), \"white\"), return_tensors=\"pt\")[\"pixel_values\"].to(DEVICE)\n",
    "        feat = model_fb.get_image_features(pixel_values=dummy_px)\n",
    "        feat_dim = feat.shape[-1]\n",
    "    head = nn.Linear(feat_dim, num_classes).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(list(head.parameters()) + list(filter(lambda p: p.requires_grad, model_fb.parameters())), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if train_loader is None:\n",
    "        print('No training loader available for fallback; exiting fallback')\n",
    "    else:\n",
    "        fallback_train_losses, fallback_train_accuracies = [], []\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            model_fb.train(); head.train()\n",
    "            running_loss = 0.0\n",
    "            running_correct = 0\n",
    "            seen = 0\n",
    "            for batch in tqdm(train_loader, desc=f\"Fallback Epoch {epoch+1}/{NUM_EPOCHS} [Train]\"):\n",
    "                if isinstance(batch, dict):\n",
    "                    imgs, labels = batch['pixel_values'], batch['label']\n",
    "                else:\n",
    "                    imgs, labels = batch\n",
    "                if isinstance(imgs, list):\n",
    "                    imgs = torch.stack(imgs, dim=0)\n",
    "                imgs = imgs.to(DEVICE); labels = labels.to(DEVICE)\n",
    "                feats = model_fb.get_image_features(pixel_values=imgs)\n",
    "                feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "                logits = head(feats)\n",
    "                preds = logits.argmax(dim=-1)\n",
    "                loss = criterion(logits, labels)\n",
    "                optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "                running_loss += loss.item()*labels.size(0)\n",
    "                running_correct += (preds == labels).sum().item()\n",
    "                seen += labels.size(0)\n",
    "            train_loss = running_loss / max(1, seen)\n",
    "            train_acc = running_correct / max(1, seen)\n",
    "            fallback_train_losses.append(train_loss)\n",
    "            fallback_train_accuracies.append(train_acc)\n",
    "            print(f\"Fallback epoch {epoch+1} - Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "        save_training_artifacts(\"finetuning_birds\", \"Fallback Fine-Tuning Birds\", fallback_train_losses, fallback_train_accuracies)\n",
    "        # Evaluate fallback\n",
    "        def eval_fb():\n",
    "            head.eval(); model_fb.eval()\n",
    "            total, correct = 0, 0\n",
    "            loader = val_loader if val_loader is not None else test_loader\n",
    "            with torch.no_grad():\n",
    "                for batch in loader:\n",
    "                    if isinstance(batch, dict):\n",
    "                        imgs, labels = batch['pixel_values'], batch['label']\n",
    "                    else:\n",
    "                        imgs, labels = batch\n",
    "                    if isinstance(imgs, list):\n",
    "                        imgs = torch.stack(imgs, dim=0)\n",
    "                    imgs = imgs.to(DEVICE); labels = labels.to(DEVICE)\n",
    "                    feats = model_fb.get_image_features(pixel_values=imgs)\n",
    "                    feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "                    preds = head(feats).argmax(dim=-1)\n",
    "                    correct += (preds == labels).sum().item(); total += labels.size(0)\n",
    "            return correct/total\n",
    "        print(f\"Fallback Val Acc: {eval_fb()*100:.2f}%\")\n",
    "\n",
    "if variant_backbone is not None:\n",
    "    register_trained_variant(\n",
    "        name=\"lora_birds\",\n",
    "        dataset=\"cub_bird\",\n",
    "        backbone=variant_backbone,\n",
    "        head=head,\n",
    "        baseline=model,\n",
    "        notes=variant_notes\n",
    "    )\n",
    "print('LoRA Birds finished')\n",
    "\n",
    "\n",
    "free_gpu_safe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4f1560",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1220719/3945290448.py:138: UserWarning: This figure was using a layout engine that is incompatible with subplots_adjust and/or tight_layout; not calling subplots_adjust.\n",
      "  fig.subplots_adjust(wspace=0.18, hspace=0.28)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved comparison grid to ./data/fine_birds_result.png\n"
     ]
    }
   ],
   "source": [
    "# LoRA 微調後（CUB-200-2011）的預測比較\n",
    "save_prediction_comparison_grid(\n",
    "    \"lora_birds\",\n",
    "    dataset=\"cub_bird\",\n",
    "    num_images=16,\n",
    "    indices=[25, 45, 123, 125, 128, 129, 140, 143, 887, 888, 1384, 1385, 2985, 4010, 5569, 5570],\n",
    "    split=\"test\",\n",
    "    cols=4,\n",
    "    top_k=5,\n",
    "    output_path=\"./data/fine_birds_result.png\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca7f155",
   "metadata": {},
   "outputs": [],
   "source": [
    "free_gpu_safe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
