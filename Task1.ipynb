{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81afeadc",
   "metadata": {},
   "source": [
    "## Task 1 - Zero-Shot Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8eb2e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python : 3.10.19 | packaged by conda-forge | (main, Oct 22 2025, 22:29:10) [GCC 14.3.0]\n",
      "CUDA   : 12.1\n",
      "Torch  : 2.3.1+cu121\n",
      "Device : NVIDIA GeForce RTX 3080\n",
      "Sun Nov  9 17:11:29 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3080        On  | 00000000:01:00.0 Off |                  N/A |\n",
      "|  0%   49C    P8              24W / 320W |     25MiB / 10240MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1616      G   /usr/lib/xorg/Xorg                            9MiB |\n",
      "|    0   N/A  N/A      2430      G   /usr/bin/gnome-shell                          6MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "#@title GPU / Python / Torch sanity\n",
    "import os, sys, subprocess, json, platform, torch\n",
    "print(\"Python :\", sys.version)\n",
    "print(\"CUDA   :\", torch.version.cuda)\n",
    "print(\"Torch  :\", torch.__version__)\n",
    "print(\"Device :\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n",
    "!nvidia-smi || true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dab32194",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abat/conda_envs/clip/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# some imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from transformers import CLIPProcessor, CLIPModel, CLIPVisionModel, logging\n",
    "import clip\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from torchinfo import summary\n",
    "from tqdm.autonotebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import json\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d0a8323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# some settings\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_ID = \"openai/clip-vit-large-patch14\" # pre-trained CLIP model (ViT-L/14)\n",
    "BATCH_SIZE = 256 # adjust based on your GPU memory\n",
    "gradient_accumulation_steps = 1 # adjust based on your GPU memory\n",
    "# For Linear Probe & LoRA\n",
    "NUM_EPOCHS = 200\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "DATA_FOLDER = \"./data\"  # folder to store datasets\n",
    "os.makedirs(DATA_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53344f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abat/conda_envs/clip/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# CLIP settings\n",
    "# --- Load CLIP Processor ---\n",
    "processor = CLIPProcessor.from_pretrained(MODEL_ID)\n",
    "# --- Define a transform to process images for CLIP ---\n",
    "class CLIPTransform:\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "\n",
    "    def __call__(self, image):\n",
    "        # The processor expects a PIL image or list of images\n",
    "        # It returns a dict, we extract 'pixel_values'\n",
    "        # .squeeze(0) removes the batch dimension the processor adds\n",
    "        return self.processor(images=image, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n",
    "\n",
    "clip_transform = CLIPTransform(processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6365bb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test samples: 6149\n",
      "Total test samples: 5794\n"
     ]
    }
   ],
   "source": [
    "# dataset related imports\n",
    "from torchvision.datasets import Flowers102 \n",
    "from datasets import load_dataset\n",
    "\n",
    "# --- Flowers102 ---\n",
    "# prepare Flowers102 dataset\n",
    "flowers102_test_dts = Flowers102(root=DATA_FOLDER, split=\"test\", transform=clip_transform, download=True) # evaluation on this set\n",
    "print(f\"Total test samples: {len(flowers102_test_dts)}\") # should be 6149\n",
    "\n",
    "# prepare class names for Flowers102\n",
    "with open(\"./data/cat_to_name.json\", \"r\") as f:\n",
    "    flowers102_class_names = json.load(f)\n",
    "\n",
    "# --- CUB-200-2011 ---\n",
    "birds_200 = load_dataset(\"bentrevett/caltech-ucsd-birds-200-2011\", cache_dir=DATA_FOLDER, download_mode=\"reuse_dataset_if_exists\")\n",
    "birds_200 = birds_200.with_transform(lambda x: processor(images=x[\"image\"], return_tensors=\"pt\"))\n",
    "\n",
    "cub_bird_test_dts = birds_200[\"test\"]\n",
    "print(f\"Total test samples: {len(cub_bird_test_dts)}\") # should be 5794\n",
    "\n",
    "# prepare class names for CUB-200-2011\n",
    "cub_bird_class_names = cub_bird_test_dts.features[\"label\"].names\n",
    "\n",
    "# === Create DataLoaders ===\n",
    "flowers102_test_loader = DataLoader(\n",
    "    flowers102_test_dts, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True\n",
    ")\n",
    "cub_bird_test_loader = DataLoader(\n",
    "    cub_bird_test_dts, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18e8d754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Method 1: Zero-Shot Classification ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Starting Method 1: Zero-Shot Classification ---\")\n",
    "\n",
    "# === 1. Load the full CLIP model ===\n",
    "model = CLIPModel.from_pretrained(MODEL_ID).to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# === 2. Create and encode text prompts ===\n",
    "# handcrafted prompts and custom prompts\n",
    "prompt_templates = [\n",
    "    \"a photo of a {}.\",\n",
    "    \"a photo of {}.\",\n",
    "    \"photo of {}.\",\n",
    "    \"a image of a {}.\",\n",
    "    \"a image of {}.\",\n",
    "    \"image of {}.\"\n",
    "]\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_text_prompts(class_names):\n",
    "    all_features = []\n",
    "    for cname in class_names:\n",
    "        texts = [t.format(cname) for t in prompt_templates]\n",
    "        # 這裡改成 Hugging Face 處理方式\n",
    "        inputs = processor(text=texts, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "        feats = model.get_text_features(**inputs)\n",
    "        feats /= feats.norm(dim=-1, keepdim=True)\n",
    "        mean_feat = feats.mean(dim=0)\n",
    "        mean_feat /= mean_feat.norm()\n",
    "        all_features.append(mean_feat)\n",
    "    return torch.stack(all_features, dim=0)\n",
    "\n",
    "flowers_text_features = encode_text_prompts(flowers102_class_names)\n",
    "cub_text_features = encode_text_prompts(cub_bird_class_names)\n",
    "\n",
    "\n",
    "# === 3. Evaluate on the test set ===\n",
    "\n",
    "@torch.no_grad()\n",
    "def zeroshot_eval(dataloader, text_features):\n",
    "    correct, total = 0, 0\n",
    "    for batch in tqdm(dataloader, desc=\"Zero-Shot Evaluation\"):\n",
    "        if isinstance(batch, dict):\n",
    "            images, labels = batch[\"pixel_values\"], batch[\"labels\"]\n",
    "        else:\n",
    "            images, labels = batch\n",
    "        if isinstance(images, list):\n",
    "            images = torch.stack(images, dim=0)\n",
    "\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        feats = model.get_image_features(images)\n",
    "        feats /= feats.norm(dim=-1, keepdim=True)\n",
    "        logits = 100.0 * feats @ text_features.T\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.numel()\n",
    "    return correct / total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f0bf5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Zero-Shot Evaluation:   0%|          | 0/25 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Zero-Shot Evaluation:   0%|          | 0/25 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/abat/conda_envs/clip/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/abat/conda_envs/clip/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/abat/conda_envs/clip/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/abat/conda_envs/clip/lib/python3.10/site-packages/torchvision/datasets/flowers102.py\", line 84, in __getitem__\n    image = self.transform(image)\nTypeError: object() takes no arguments\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 3\u001b[0m     flowers102_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mzeroshot_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflowers102_test_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflowers_text_features\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda_envs/clip/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 41\u001b[0m, in \u001b[0;36mzeroshot_eval\u001b[0;34m(dataloader, text_features)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mzeroshot_eval\u001b[39m(dataloader, text_features):\n\u001b[1;32m     40\u001b[0m     correct, total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZero-Shot Evaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m     43\u001b[0m             images, labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m], batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/conda_envs/clip/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/conda_envs/clip/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/conda_envs/clip/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda_envs/clip/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1372\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/conda_envs/clip/lib/python3.10/site-packages/torch/_utils.py:705\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 705\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/abat/conda_envs/clip/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/abat/conda_envs/clip/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/abat/conda_envs/clip/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/abat/conda_envs/clip/lib/python3.10/site-packages/torchvision/datasets/flowers102.py\", line 84, in __getitem__\n    image = self.transform(image)\nTypeError: object() takes no arguments\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    flowers102_accuracy = zeroshot_eval(flowers102_test_loader, flowers_text_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecceb02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    cub_bird_accuracy = zeroshot_eval(cub_bird_test_loader, cub_text_features)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0a71dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 4. Result Analysis ===\n",
    "\n",
    "print(f\"\\nZero-Shot Test Accuracy: {flowers102_accuracy * 100:.2f}%\")\n",
    "\n",
    "print(f\"\\nZero-Shot Test Accuracy: {cub_bird_accuracy * 100:.2f}%\")\n",
    "\n",
    "# also can do the \"classification_report\" and \"confusion_matrix\" here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f42850a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 5. Visualization ===\n",
    "# use plt to visualize some predictions\n",
    " \n",
    "def visualize_predictions(dataloader, text_features, class_names, num_images=6):\n",
    "    model.eval()\n",
    "    images_list, labels_list, preds_list = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            if isinstance(batch, dict):\n",
    "                images, labels = batch[\"pixel_values\"], batch[\"labels\"]\n",
    "            else:\n",
    "                images, labels = batch\n",
    "            if isinstance(images, list):\n",
    "                images = torch.stack(images, dim=0)\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            feats = model.encode_image(images)\n",
    "            feats /= feats.norm(dim=-1, keepdim=True)\n",
    "            logits = 100.0 * feats @ text_features.T\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            images_list.append(images.cpu())\n",
    "            labels_list.append(labels.cpu())\n",
    "            preds_list.append(preds.cpu())\n",
    "            if len(images_list) > 1:\n",
    "                break\n",
    "    images = torch.cat(images_list)[:num_images]\n",
    "    labels = torch.cat(labels_list)[:num_images]\n",
    "    preds = torch.cat(preds_list)[:num_images]\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(2, (num_images + 1) // 2, i + 1)\n",
    "        img = images[i].permute(1, 2, 0).numpy()\n",
    "        img = (img - img.min()) / (img.max() - img.min() + 1e-6)\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"GT: {class_names[labels[i]]}\\nPred: {class_names[preds[i]]}\", fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# visualize some flowers predictions\n",
    "visualize_predictions(flowers102_test_loader, flowers_text_features, flowers102_class_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
