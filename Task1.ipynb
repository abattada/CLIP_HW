{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81afeadc",
   "metadata": {},
   "source": [
    "## Task 1 - Zero-Shot Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8eb2e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python : 3.10.19 | packaged by conda-forge | (main, Oct 22 2025, 22:29:10) [GCC 14.3.0]\n",
      "CUDA   : 12.1\n",
      "Torch  : 2.3.1+cu121\n",
      "Device : NVIDIA GeForce RTX 3080\n",
      "Sun Nov  9 17:11:29 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3080        On  | 00000000:01:00.0 Off |                  N/A |\n",
      "|  0%   49C    P8              24W / 320W |     25MiB / 10240MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1616      G   /usr/lib/xorg/Xorg                            9MiB |\n",
      "|    0   N/A  N/A      2430      G   /usr/bin/gnome-shell                          6MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "#@title GPU / Python / Torch sanity\n",
    "import os, sys, subprocess, json, platform, torch\n",
    "print(\"Python :\", sys.version)\n",
    "print(\"CUDA   :\", torch.version.cuda)\n",
    "print(\"Torch  :\", torch.__version__)\n",
    "print(\"Device :\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n",
    "!nvidia-smi || true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dab32194",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abat/conda_envs/clip/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# some imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from transformers import CLIPProcessor, CLIPModel, CLIPVisionModel, logging\n",
    "import clip\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from torchinfo import summary\n",
    "from tqdm.autonotebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import json\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d0a8323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# some settings\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_ID = \"openai/clip-vit-large-patch14\" # pre-trained CLIP model (ViT-L/14)\n",
    "BATCH_SIZE = 256 # adjust based on your GPU memory\n",
    "gradient_accumulation_steps = 1 # adjust based on your GPU memory\n",
    "# For Linear Probe & LoRA\n",
    "NUM_EPOCHS = 200\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "DATA_FOLDER = \"./data\"  # folder to store datasets\n",
    "os.makedirs(DATA_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53344f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abat/conda_envs/clip/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# CLIP settings\n",
    "# --- Load CLIP Processor ---\n",
    "processor = CLIPProcessor.from_pretrained(MODEL_ID)\n",
    "# --- Define a transform to process images for CLIP ---\n",
    "class CLIPTransform:\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "\n",
    "    def __call__(self, image):\n",
    "        # The processor expects a PIL image or list of images\n",
    "        # It returns a dict, we extract 'pixel_values'\n",
    "        # .squeeze(0) removes the batch dimension the processor adds\n",
    "        return self.processor(images=image, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n",
    "\n",
    "clip_transform = CLIPTransform(processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6365bb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test samples: 6149\n",
      "Total test samples: 5794\n",
      "['001.Black_footed_Albatross', '002.Laysan_Albatross', '003.Sooty_Albatross', '004.Groove_billed_Ani', '005.Crested_Auklet', '006.Least_Auklet', '007.Parakeet_Auklet', '008.Rhinoceros_Auklet', '009.Brewer_Blackbird', '010.Red_winged_Blackbird', '011.Rusty_Blackbird', '012.Yellow_headed_Blackbird', '013.Bobolink', '014.Indigo_Bunting', '015.Lazuli_Bunting', '016.Painted_Bunting', '017.Cardinal', '018.Spotted_Catbird', '019.Gray_Catbird', '020.Yellow_breasted_Chat', '021.Eastern_Towhee', '022.Chuck_will_Widow', '023.Brandt_Cormorant', '024.Red_faced_Cormorant', '025.Pelagic_Cormorant', '026.Bronzed_Cowbird', '027.Shiny_Cowbird', '028.Brown_Creeper', '029.American_Crow', '030.Fish_Crow', '031.Black_billed_Cuckoo', '032.Mangrove_Cuckoo', '033.Yellow_billed_Cuckoo', '034.Gray_crowned_Rosy_Finch', '035.Purple_Finch', '036.Northern_Flicker', '037.Acadian_Flycatcher', '038.Great_Crested_Flycatcher', '039.Least_Flycatcher', '040.Olive_sided_Flycatcher', '041.Scissor_tailed_Flycatcher', '042.Vermilion_Flycatcher', '043.Yellow_bellied_Flycatcher', '044.Frigatebird', '045.Northern_Fulmar', '046.Gadwall', '047.American_Goldfinch', '048.European_Goldfinch', '049.Boat_tailed_Grackle', '050.Eared_Grebe', '051.Horned_Grebe', '052.Pied_billed_Grebe', '053.Western_Grebe', '054.Blue_Grosbeak', '055.Evening_Grosbeak', '056.Pine_Grosbeak', '057.Rose_breasted_Grosbeak', '058.Pigeon_Guillemot', '059.California_Gull', '060.Glaucous_winged_Gull', '061.Heermann_Gull', '062.Herring_Gull', '063.Ivory_Gull', '064.Ring_billed_Gull', '065.Slaty_backed_Gull', '066.Western_Gull', '067.Anna_Hummingbird', '068.Ruby_throated_Hummingbird', '069.Rufous_Hummingbird', '070.Green_Violetear', '071.Long_tailed_Jaeger', '072.Pomarine_Jaeger', '073.Blue_Jay', '074.Florida_Jay', '075.Green_Jay', '076.Dark_eyed_Junco', '077.Tropical_Kingbird', '078.Gray_Kingbird', '079.Belted_Kingfisher', '080.Green_Kingfisher', '081.Pied_Kingfisher', '082.Ringed_Kingfisher', '083.White_breasted_Kingfisher', '084.Red_legged_Kittiwake', '085.Horned_Lark', '086.Pacific_Loon', '087.Mallard', '088.Western_Meadowlark', '089.Hooded_Merganser', '090.Red_breasted_Merganser', '091.Mockingbird', '092.Nighthawk', '093.Clark_Nutcracker', '094.White_breasted_Nuthatch', '095.Baltimore_Oriole', '096.Hooded_Oriole', '097.Orchard_Oriole', '098.Scott_Oriole', '099.Ovenbird', '100.Brown_Pelican', '101.White_Pelican', '102.Western_Wood_Pewee', '103.Sayornis', '104.American_Pipit', '105.Whip_poor_Will', '106.Horned_Puffin', '107.Common_Raven', '108.White_necked_Raven', '109.American_Redstart', '110.Geococcyx', '111.Loggerhead_Shrike', '112.Great_Grey_Shrike', '113.Baird_Sparrow', '114.Black_throated_Sparrow', '115.Brewer_Sparrow', '116.Chipping_Sparrow', '117.Clay_colored_Sparrow', '118.House_Sparrow', '119.Field_Sparrow', '120.Fox_Sparrow', '121.Grasshopper_Sparrow', '122.Harris_Sparrow', '123.Henslow_Sparrow', '124.Le_Conte_Sparrow', '125.Lincoln_Sparrow', '126.Nelson_Sharp_tailed_Sparrow', '127.Savannah_Sparrow', '128.Seaside_Sparrow', '129.Song_Sparrow', '130.Tree_Sparrow', '131.Vesper_Sparrow', '132.White_crowned_Sparrow', '133.White_throated_Sparrow', '134.Cape_Glossy_Starling', '135.Bank_Swallow', '136.Barn_Swallow', '137.Cliff_Swallow', '138.Tree_Swallow', '139.Scarlet_Tanager', '140.Summer_Tanager', '141.Artic_Tern', '142.Black_Tern', '143.Caspian_Tern', '144.Common_Tern', '145.Elegant_Tern', '146.Forsters_Tern', '147.Least_Tern', '148.Green_tailed_Towhee', '149.Brown_Thrasher', '150.Sage_Thrasher', '151.Black_capped_Vireo', '152.Blue_headed_Vireo', '153.Philadelphia_Vireo', '154.Red_eyed_Vireo', '155.Warbling_Vireo', '156.White_eyed_Vireo', '157.Yellow_throated_Vireo', '158.Bay_breasted_Warbler', '159.Black_and_white_Warbler', '160.Black_throated_Blue_Warbler', '161.Blue_winged_Warbler', '162.Canada_Warbler', '163.Cape_May_Warbler', '164.Cerulean_Warbler', '165.Chestnut_sided_Warbler', '166.Golden_winged_Warbler', '167.Hooded_Warbler', '168.Kentucky_Warbler', '169.Magnolia_Warbler', '170.Mourning_Warbler', '171.Myrtle_Warbler', '172.Nashville_Warbler', '173.Orange_crowned_Warbler', '174.Palm_Warbler', '175.Pine_Warbler', '176.Prairie_Warbler', '177.Prothonotary_Warbler', '178.Swainson_Warbler', '179.Tennessee_Warbler', '180.Wilson_Warbler', '181.Worm_eating_Warbler', '182.Yellow_Warbler', '183.Northern_Waterthrush', '184.Louisiana_Waterthrush', '185.Bohemian_Waxwing', '186.Cedar_Waxwing', '187.American_Three_toed_Woodpecker', '188.Pileated_Woodpecker', '189.Red_bellied_Woodpecker', '190.Red_cockaded_Woodpecker', '191.Red_headed_Woodpecker', '192.Downy_Woodpecker', '193.Bewick_Wren', '194.Cactus_Wren', '195.Carolina_Wren', '196.House_Wren', '197.Marsh_Wren', '198.Rock_Wren', '199.Winter_Wren', '200.Common_Yellowthroat']\n"
     ]
    }
   ],
   "source": [
    "# dataset related imports\n",
    "from torchvision.datasets import Flowers102 \n",
    "from datasets import load_dataset\n",
    "\n",
    "# --- Flowers102 ---\n",
    "# prepare Flowers102 dataset\n",
    "flowers102_test_dts = Flowers102(root=DATA_FOLDER, split=\"test\", transform=clip_transform, download=True) # evaluation on this set\n",
    "print(f\"Total test samples: {len(flowers102_test_dts)}\") # should be 6149\n",
    "\n",
    "# prepare class names for Flowers102\n",
    "with open(\"./data/cat_to_name.json\", \"r\") as f:\n",
    "    flowers102_class_names = json.load(f)\n",
    "\n",
    "# --- CUB-200-2011 ---\n",
    "birds_200 = load_dataset(\"bentrevett/caltech-ucsd-birds-200-2011\", cache_dir=DATA_FOLDER, download_mode=\"reuse_dataset_if_exists\")\n",
    "birds_200 = birds_200.with_transform(lambda x: processor(images=x[\"image\"], return_tensors=\"pt\"))\n",
    "\n",
    "cub_bird_test_dts = birds_200[\"test\"]\n",
    "print(f\"Total test samples: {len(cub_bird_test_dts)}\") # should be 5794\n",
    "\n",
    "# prepare class names for CUB-200-2011\n",
    "cub_bird_class_names = cub_bird_test_dts.features[\"label\"].names\n",
    "\n",
    "# === Create DataLoaders ===\n",
    "flowers102_test_loader = DataLoader(\n",
    "    flowers102_test_dts, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True\n",
    ")\n",
    "cub_bird_test_loader = DataLoader(\n",
    "    cub_bird_test_dts, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e8d754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Method 1: Zero-Shot Classification ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Starting Method 1: Zero-Shot Classification ---\")\n",
    "\n",
    "# === 1. Load the full CLIP model ===\n",
    "from transformers import CLIPModel, AutoProcessor\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "model = CLIPModel.from_pretrained(MODEL_ID).to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# === 2. Create and encode text prompts ===\n",
    "prompt_templates = [\n",
    "    \"a photo of a {}.\",\n",
    "    \"a photo of {}.\",\n",
    "    \"photo of {}.\",\n",
    "    \"an image of a {}.\",\n",
    "    \"an image of {}.\",\n",
    "    \"image of {}.\"\n",
    "]\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_text_prompts(class_names):\n",
    "    all_features = []\n",
    "    for cname in class_names:\n",
    "        texts = [t.format(cname) for t in prompt_templates]\n",
    "        inputs = processor(text=texts, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "        feats = model.get_text_features(**inputs)\n",
    "        feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "        mean_feat = feats.mean(dim=0)\n",
    "        mean_feat = mean_feat / mean_feat.norm()\n",
    "        all_features.append(mean_feat)\n",
    "    return torch.stack(all_features, dim=0)\n",
    "\n",
    "flowers_text_features = encode_text_prompts(flowers102_class_names)\n",
    "cub_text_features = encode_text_prompts(cub_bird_class_names)\n",
    "\n",
    "# === 3. Evaluate on the test set ===\n",
    "@torch.no_grad()\n",
    "def zeroshot_eval(dataloader, text_features):\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Zero-Shot Evaluation\"):\n",
    "        # ---- 抽取 images ----\n",
    "        if isinstance(batch, dict):\n",
    "            if \"pixel_values\" in batch:\n",
    "                images = batch[\"pixel_values\"]\n",
    "            elif \"image\" in batch:  # HF Datasets 可能給 PIL images\n",
    "                proc = processor(images=batch[\"image\"], return_tensors=\"pt\")\n",
    "                images = proc[\"pixel_values\"]\n",
    "            else:\n",
    "                raise KeyError(f\"batch keys {list(batch.keys())} 不含 pixel_values/image\")\n",
    "            # ---- 抽取 labels（容忍 label / labels 兩種）----\n",
    "            if \"labels\" in batch:\n",
    "                labels = batch[\"labels\"]\n",
    "            elif \"label\" in batch:\n",
    "                labels = batch[\"label\"]\n",
    "            else:\n",
    "                raise KeyError(f\"batch keys {list(batch.keys())} 不含 label/labels\")\n",
    "        else:\n",
    "            # torchvision Dataset: (images, labels)\n",
    "            images, labels = batch\n",
    "\n",
    "        if isinstance(images, list):\n",
    "            images = torch.stack(images, dim=0)\n",
    "\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        image_feats = model.get_image_features(pixel_values=images)\n",
    "        image_feats = image_feats / image_feats.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        logits = 100.0 * image_feats @ text_features.T\n",
    "        preds = logits.argmax(dim=-1)\n",
    "\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.numel()\n",
    "\n",
    "    return correct / max(total, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f0bf5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Zero-Shot Evaluation:   0%|          | 0/25 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Zero-Shot Evaluation: 100%|██████████| 25/25 [01:14<00:00,  2.98s/it]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    flowers102_accuracy = zeroshot_eval(flowers102_test_loader, flowers_text_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecceb02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Zero-Shot Evaluation:   0%|          | 0/23 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "Zero-Shot Evaluation:   0%|          | 0/23 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 2\u001b[0m     cub_bird_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mzeroshot_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcub_bird_test_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcub_text_features\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda_envs/clip/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 43\u001b[0m, in \u001b[0;36mzeroshot_eval\u001b[0;34m(dataloader, text_features)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZero-Shot Evaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m---> 43\u001b[0m         images, labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     45\u001b[0m         images, labels \u001b[38;5;241m=\u001b[39m batch\n",
      "\u001b[0;31mKeyError\u001b[0m: 'labels'"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    cub_bird_accuracy = zeroshot_eval(cub_bird_test_loader, cub_text_features)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0a71dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 4. Result Analysis ===\n",
    "\n",
    "print(f\"\\nZero-Shot Test Accuracy: {flowers102_accuracy * 100:.2f}%\")\n",
    "\n",
    "print(f\"\\nZero-Shot Test Accuracy: {cub_bird_accuracy * 100:.2f}%\")\n",
    "\n",
    "# also can do the \"classification_report\" and \"confusion_matrix\" here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f42850a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 5. Visualization ===\n",
    "# use plt to visualize some predictions\n",
    " \n",
    "def visualize_predictions(dataloader, text_features, class_names, num_images=6):\n",
    "    model.eval()\n",
    "    images_list, labels_list, preds_list = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            if isinstance(batch, dict):\n",
    "                images, labels = batch[\"pixel_values\"], batch[\"labels\"]\n",
    "            else:\n",
    "                images, labels = batch\n",
    "            if isinstance(images, list):\n",
    "                images = torch.stack(images, dim=0)\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            feats = model.encode_image(images)\n",
    "            feats /= feats.norm(dim=-1, keepdim=True)\n",
    "            logits = 100.0 * feats @ text_features.T\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            images_list.append(images.cpu())\n",
    "            labels_list.append(labels.cpu())\n",
    "            preds_list.append(preds.cpu())\n",
    "            if len(images_list) > 1:\n",
    "                break\n",
    "    images = torch.cat(images_list)[:num_images]\n",
    "    labels = torch.cat(labels_list)[:num_images]\n",
    "    preds = torch.cat(preds_list)[:num_images]\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(2, (num_images + 1) // 2, i + 1)\n",
    "        img = images[i].permute(1, 2, 0).numpy()\n",
    "        img = (img - img.min()) / (img.max() - img.min() + 1e-6)\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"GT: {class_names[labels[i]]}\\nPred: {class_names[preds[i]]}\", fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# visualize some flowers predictions\n",
    "visualize_predictions(flowers102_test_loader, flowers_text_features, flowers102_class_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
